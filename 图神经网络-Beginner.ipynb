{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9cb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch_sparse\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid, Flickr\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv\n",
    "from torch.nn import Linear, Sequential, ReLU\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.datasets import TUDataset, ZINC\n",
    "from torch_geometric.transforms import NormalizeFeatures, ToUndirected\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling, remove_self_loops, add_self_loops, to_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373e4457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.1.0\n",
      "torch-sparse: 0.6.18+pt21cpu\n",
      "torch-geometric: 2.6.1\n",
      "✅ NeighborLoader 正常可用\n"
     ]
    }
   ],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torch-sparse:\", torch_sparse.__version__)\n",
    "print(\"torch-geometric:\", torch_geometric.__version__)\n",
    "print(\"✅ NeighborLoader 正常可用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be323a8c",
   "metadata": {},
   "source": [
    "# 节点分类\n",
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0a641e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''标准图卷积层'''\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x, edge_index))\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return self.layers[-1](x, edge_index)\n",
    "'''使用注意力机制计算邻居节点的加权平均'''\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, 0.5, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "'''可采样邻居'''\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, 0.5, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "'''强调结构区分能力，更复杂的 MLP 替代单线性变换'''\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        nn1 = Sequential(Linear(in_channels, hidden_channels), ReLU(), Linear(hidden_channels, hidden_channels))\n",
    "        nn2 = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels, out_channels))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.conv2 = GINConv(nn2)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, 0.5, training=self.training)\n",
    "        return self.conv2(x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66137175",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f83643f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalOnlyPlanetoid(Planetoid):\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        if self.name.lower() == 'cora':\n",
    "            return ['cora.content', 'cora.cites']\n",
    "        elif self.name.lower() == 'citeseer':\n",
    "            return ['citeseer.content', 'citeseer.cites']\n",
    "        elif self.name.lower() == 'pubmed':\n",
    "            return ['Pubmed-Diabetes.NODE.paper.tab', 'Pubmed-Diabetes.DIRECTED.cites.tab']\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset: {self.name}\")\n",
    "    def download(self):\n",
    "        print(f\"\\nChecking dataset: {self.name}\")\n",
    "        print(f\"Root directory: {self.root}\")\n",
    "        print(f\"Raw directory: {self.raw_dir}\")\n",
    "        print(f\"Expected raw files: {self.raw_file_names}\")\n",
    "        if os.path.exists(self.raw_dir):\n",
    "            print(f\"Files in raw directory: {os.listdir(self.raw_dir)}\")\n",
    "        else:\n",
    "            print(\"Raw directory does not exist!\")\n",
    "        missing_files = [f for f in self.raw_paths if not os.path.exists(f)]\n",
    "        if missing_files:\n",
    "            raise RuntimeError(\n",
    "                f\"Missing {len(missing_files)} files in {self.raw_dir}:\\n\"\n",
    "                f\"- Missing: {missing_files}\\n\"\n",
    "                f\"- Required: {self.raw_file_names}\\n\"\n",
    "                \"Please download them from:\\n\"\n",
    "                \"- Cora/Citeseer: https://linqs-data.soe.ucsc.edu/public/lbc/\\n\"\n",
    "                \"- Pubmed: https://linqs-data.soe.ucsc.edu/public/Pubmed-Diabetes.tgz\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"All raw files found!\")\n",
    "\n",
    "def load_data(name):\n",
    "    name = name.lower()\n",
    "    base_path = r'D:\\Data\\master\\Graph Machine Learning\\GNN\\standard benchmark datasets'\n",
    "    transform = T.NormalizeFeatures()\n",
    "    # 直接使用数据集父目录作为root，并小写化name\n",
    "    if name == 'cora':\n",
    "        return LocalOnlyPlanetoid(root=os.path.join(base_path, ''), name='cora', transform=transform)\n",
    "    elif name == 'citeseer':\n",
    "        return LocalOnlyPlanetoid(root=os.path.join(base_path, ''), name='citeseer', transform=transform)\n",
    "    elif name == 'flickr':\n",
    "        return Flickr(root=os.path.join(base_path, ''), transform=transform)\n",
    "    else:\n",
    "        raise ValueError(\"Only 'cora', 'citeseer', and 'flickr' are supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4df954",
   "metadata": {},
   "source": [
    "## 图训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16a48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''全图训练'''\n",
    "def train_full(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "'''子图采样训练'''\n",
    "def train_sample(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = F.cross_entropy(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fdf0d",
   "metadata": {},
   "source": [
    "## 主程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2770031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Cora] GCN | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9463, Acc [0.3, 0.184, 0.203]\n",
      "Epoch 10: Loss 1.8533, Acc [0.9142857142857143, 0.666, 0.692]\n",
      "Epoch 20: Loss 1.6965, Acc [0.95, 0.756, 0.77]\n",
      "Epoch 30: Loss 1.4786, Acc [0.9642857142857143, 0.776, 0.783]\n",
      "Epoch 40: Loss 1.2311, Acc [0.9714285714285714, 0.79, 0.796]\n",
      "Epoch 50: Loss 1.0170, Acc [0.9785714285714285, 0.8, 0.814]\n",
      "Epoch 60: Loss 0.7971, Acc [0.9785714285714285, 0.802, 0.815]\n",
      "Epoch 70: Loss 0.6713, Acc [0.9857142857142858, 0.802, 0.821]\n",
      "Epoch 80: Loss 0.5500, Acc [0.9857142857142858, 0.802, 0.823]\n",
      "Epoch 90: Loss 0.4961, Acc [0.9857142857142858, 0.802, 0.814]\n",
      "Total time: 2.22s\n",
      "\n",
      "--- [Cora] GCN | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9453, Acc [0.6142857142857143, 0.454, 0.482]\n",
      "Epoch 10: Loss 1.8116, Acc [0.9428571428571428, 0.768, 0.789]\n",
      "Epoch 20: Loss 1.5875, Acc [0.95, 0.768, 0.789]\n",
      "Epoch 30: Loss 1.3347, Acc [0.9714285714285714, 0.778, 0.801]\n",
      "Epoch 40: Loss 1.0345, Acc [0.9642857142857143, 0.788, 0.81]\n",
      "Epoch 50: Loss 0.8105, Acc [0.9714285714285714, 0.792, 0.813]\n",
      "Epoch 60: Loss 0.6143, Acc [0.9785714285714285, 0.808, 0.822]\n",
      "Epoch 70: Loss 0.4876, Acc [0.9785714285714285, 0.802, 0.821]\n",
      "Epoch 80: Loss 0.4329, Acc [0.9928571428571429, 0.802, 0.821]\n",
      "Epoch 90: Loss 0.3697, Acc [0.9928571428571429, 0.802, 0.826]\n",
      "Total time: 1.83s\n",
      "\n",
      "--- [Cora] GCN | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.9470, Acc [0.5428571428571428, 0.35, 0.321]\n",
      "Epoch 10: Loss 1.6886, Acc [0.95, 0.77, 0.783]\n",
      "Epoch 20: Loss 1.2574, Acc [0.95, 0.782, 0.812]\n",
      "Epoch 30: Loss 0.7673, Acc [0.9785714285714285, 0.788, 0.818]\n",
      "Epoch 40: Loss 0.4786, Acc [0.9857142857142858, 0.786, 0.81]\n",
      "Epoch 50: Loss 0.3349, Acc [0.9928571428571429, 0.78, 0.808]\n",
      "Epoch 60: Loss 0.3110, Acc [0.9928571428571429, 0.782, 0.82]\n",
      "Epoch 70: Loss 0.2534, Acc [1.0, 0.782, 0.819]\n",
      "Epoch 80: Loss 0.2133, Acc [1.0, 0.78, 0.813]\n",
      "Epoch 90: Loss 0.2075, Acc [1.0, 0.786, 0.803]\n",
      "Total time: 1.79s\n",
      "\n",
      "--- [Cora] GCN | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.9469, Acc [0.2785714285714286, 0.304, 0.327]\n",
      "Epoch 10: Loss 1.8452, Acc [0.9357142857142857, 0.746, 0.758]\n",
      "Epoch 20: Loss 1.4643, Acc [0.9428571428571428, 0.762, 0.771]\n",
      "Epoch 30: Loss 0.7718, Acc [0.9428571428571428, 0.776, 0.795]\n",
      "Epoch 40: Loss 0.3193, Acc [0.9714285714285714, 0.792, 0.827]\n",
      "Epoch 50: Loss 0.1922, Acc [0.9857142857142858, 0.798, 0.813]\n",
      "Epoch 60: Loss 0.1005, Acc [1.0, 0.794, 0.823]\n",
      "Epoch 70: Loss 0.0887, Acc [1.0, 0.788, 0.816]\n",
      "Epoch 80: Loss 0.0740, Acc [1.0, 0.79, 0.801]\n",
      "Epoch 90: Loss 0.0645, Acc [1.0, 0.79, 0.802]\n",
      "Total time: 2.00s\n",
      "\n",
      "--- [Cora] GAT | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9462, Acc [0.9214285714285714, 0.728, 0.755]\n",
      "Epoch 10: Loss 1.4791, Acc [0.9642857142857143, 0.794, 0.799]\n",
      "Epoch 20: Loss 0.8062, Acc [0.9785714285714285, 0.794, 0.793]\n",
      "Epoch 30: Loss 0.3930, Acc [0.9857142857142858, 0.782, 0.799]\n",
      "Epoch 40: Loss 0.2526, Acc [0.9928571428571429, 0.772, 0.788]\n",
      "Epoch 50: Loss 0.2028, Acc [1.0, 0.764, 0.786]\n",
      "Epoch 60: Loss 0.1709, Acc [1.0, 0.77, 0.776]\n",
      "Epoch 70: Loss 0.1450, Acc [1.0, 0.766, 0.765]\n",
      "Epoch 80: Loss 0.1280, Acc [1.0, 0.74, 0.758]\n",
      "Epoch 90: Loss 0.1105, Acc [1.0, 0.734, 0.752]\n",
      "Total time: 16.71s\n",
      "\n",
      "--- [Cora] GAT | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9468, Acc [0.8071428571428572, 0.504, 0.511]\n",
      "Epoch 10: Loss 1.4824, Acc [0.9571428571428572, 0.786, 0.786]\n",
      "Epoch 20: Loss 0.8125, Acc [0.9642857142857143, 0.786, 0.799]\n",
      "Epoch 30: Loss 0.4117, Acc [0.9785714285714285, 0.8, 0.805]\n",
      "Epoch 40: Loss 0.2972, Acc [0.9928571428571429, 0.784, 0.804]\n",
      "Epoch 50: Loss 0.2261, Acc [1.0, 0.778, 0.795]\n",
      "Epoch 60: Loss 0.1784, Acc [1.0, 0.768, 0.786]\n",
      "Epoch 70: Loss 0.1554, Acc [1.0, 0.77, 0.773]\n",
      "Epoch 80: Loss 0.1365, Acc [1.0, 0.772, 0.77]\n",
      "Epoch 90: Loss 0.1225, Acc [1.0, 0.754, 0.75]\n",
      "Total time: 8.18s\n",
      "\n",
      "--- [Cora] GAT | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.9445, Acc [0.9285714285714286, 0.732, 0.737]\n",
      "Epoch 10: Loss 0.8849, Acc [0.9714285714285714, 0.794, 0.807]\n",
      "Epoch 20: Loss 0.2555, Acc [0.9857142857142858, 0.778, 0.8]\n",
      "Epoch 30: Loss 0.1788, Acc [0.9928571428571429, 0.772, 0.79]\n",
      "Epoch 40: Loss 0.1362, Acc [1.0, 0.766, 0.778]\n",
      "Epoch 50: Loss 0.1355, Acc [1.0, 0.75, 0.759]\n",
      "Epoch 60: Loss 0.1040, Acc [1.0, 0.75, 0.771]\n",
      "Epoch 70: Loss 0.1050, Acc [1.0, 0.75, 0.772]\n",
      "Epoch 80: Loss 0.0944, Acc [1.0, 0.738, 0.751]\n",
      "Epoch 90: Loss 0.0961, Acc [1.0, 0.724, 0.749]\n",
      "Total time: 8.54s\n",
      "\n",
      "--- [Cora] GAT | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.9450, Acc [0.8, 0.492, 0.512]\n",
      "Epoch 10: Loss 1.4781, Acc [0.9571428571428572, 0.79, 0.786]\n",
      "Epoch 20: Loss 0.8163, Acc [0.9785714285714285, 0.798, 0.805]\n",
      "Epoch 30: Loss 0.4097, Acc [0.9857142857142858, 0.798, 0.811]\n",
      "Epoch 40: Loss 0.2710, Acc [0.9857142857142858, 0.804, 0.817]\n",
      "Epoch 50: Loss 0.2178, Acc [0.9928571428571429, 0.798, 0.815]\n",
      "Epoch 60: Loss 0.1869, Acc [1.0, 0.796, 0.814]\n",
      "Epoch 70: Loss 0.1542, Acc [1.0, 0.786, 0.81]\n",
      "Epoch 80: Loss 0.1442, Acc [1.0, 0.786, 0.805]\n",
      "Epoch 90: Loss 0.1291, Acc [1.0, 0.774, 0.804]\n",
      "Total time: 8.15s\n",
      "\n",
      "--- [Cora] GraphSAGE | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9490, Acc [0.14285714285714285, 0.316, 0.319]\n",
      "Epoch 10: Loss 1.7805, Acc [0.9928571428571429, 0.674, 0.646]\n",
      "Epoch 20: Loss 1.4257, Acc [0.9928571428571429, 0.748, 0.733]\n",
      "Epoch 30: Loss 0.9607, Acc [0.9928571428571429, 0.776, 0.771]\n",
      "Epoch 40: Loss 0.5258, Acc [1.0, 0.772, 0.784]\n",
      "Epoch 50: Loss 0.2960, Acc [1.0, 0.782, 0.797]\n",
      "Epoch 60: Loss 0.2082, Acc [1.0, 0.784, 0.797]\n",
      "Epoch 70: Loss 0.1742, Acc [1.0, 0.77, 0.792]\n",
      "Epoch 80: Loss 0.1527, Acc [1.0, 0.77, 0.798]\n",
      "Epoch 90: Loss 0.1411, Acc [1.0, 0.774, 0.797]\n",
      "Total time: 5.55s\n",
      "\n",
      "--- [Cora] GraphSAGE | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9483, Acc [0.14285714285714285, 0.072, 0.091]\n",
      "Epoch 10: Loss 1.7718, Acc [1.0, 0.692, 0.665]\n",
      "Epoch 20: Loss 1.4078, Acc [1.0, 0.744, 0.733]\n",
      "Epoch 30: Loss 0.9215, Acc [1.0, 0.762, 0.75]\n",
      "Epoch 40: Loss 0.5215, Acc [0.9928571428571429, 0.772, 0.779]\n",
      "Epoch 50: Loss 0.3049, Acc [1.0, 0.776, 0.783]\n",
      "Epoch 60: Loss 0.2174, Acc [1.0, 0.772, 0.793]\n",
      "Epoch 70: Loss 0.1661, Acc [1.0, 0.768, 0.793]\n",
      "Epoch 80: Loss 0.1493, Acc [1.0, 0.766, 0.793]\n",
      "Epoch 90: Loss 0.1473, Acc [1.0, 0.768, 0.801]\n",
      "Total time: 3.08s\n",
      "\n",
      "--- [Cora] GraphSAGE | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.9464, Acc [0.45714285714285713, 0.398, 0.432]\n",
      "Epoch 10: Loss 1.5012, Acc [1.0, 0.728, 0.723]\n",
      "Epoch 20: Loss 0.6937, Acc [1.0, 0.772, 0.774]\n",
      "Epoch 30: Loss 0.2256, Acc [1.0, 0.768, 0.796]\n",
      "Epoch 40: Loss 0.1412, Acc [1.0, 0.764, 0.795]\n",
      "Epoch 50: Loss 0.1306, Acc [1.0, 0.756, 0.794]\n",
      "Epoch 60: Loss 0.1085, Acc [1.0, 0.766, 0.8]\n",
      "Epoch 70: Loss 0.1054, Acc [1.0, 0.764, 0.805]\n",
      "Epoch 80: Loss 0.0989, Acc [1.0, 0.762, 0.791]\n",
      "Epoch 90: Loss 0.0810, Acc [1.0, 0.768, 0.801]\n",
      "Total time: 2.98s\n",
      "\n",
      "--- [Cora] GraphSAGE | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.9463, Acc [0.14285714285714285, 0.122, 0.13]\n",
      "Epoch 10: Loss 1.7676, Acc [1.0, 0.696, 0.655]\n",
      "Epoch 20: Loss 1.3904, Acc [1.0, 0.748, 0.752]\n",
      "Epoch 30: Loss 0.8660, Acc [1.0, 0.786, 0.778]\n",
      "Epoch 40: Loss 0.4908, Acc [0.9928571428571429, 0.79, 0.784]\n",
      "Epoch 50: Loss 0.3116, Acc [1.0, 0.78, 0.797]\n",
      "Epoch 60: Loss 0.2058, Acc [1.0, 0.78, 0.801]\n",
      "Epoch 70: Loss 0.1596, Acc [1.0, 0.77, 0.807]\n",
      "Epoch 80: Loss 0.1431, Acc [1.0, 0.764, 0.802]\n",
      "Epoch 90: Loss 0.1372, Acc [1.0, 0.756, 0.8]\n",
      "Total time: 3.01s\n",
      "\n",
      "--- [Cora] GIN | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9415, Acc [0.15, 0.13, 0.131]\n",
      "Epoch 10: Loss 1.1848, Acc [0.85, 0.722, 0.729]\n",
      "Epoch 20: Loss 0.3584, Acc [0.9642857142857143, 0.724, 0.728]\n",
      "Epoch 30: Loss 0.1037, Acc [1.0, 0.686, 0.699]\n",
      "Epoch 40: Loss 0.0337, Acc [1.0, 0.738, 0.732]\n",
      "Epoch 50: Loss 0.0471, Acc [1.0, 0.67, 0.674]\n",
      "Epoch 60: Loss 0.0954, Acc [1.0, 0.696, 0.706]\n",
      "Epoch 70: Loss 0.0300, Acc [1.0, 0.75, 0.769]\n",
      "Epoch 80: Loss 0.0109, Acc [1.0, 0.72, 0.732]\n",
      "Epoch 90: Loss 0.0216, Acc [1.0, 0.74, 0.742]\n",
      "Total time: 4.85s\n",
      "\n",
      "--- [Cora] GIN | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9510, Acc [0.15, 0.124, 0.136]\n",
      "Epoch 10: Loss 1.3092, Acc [0.8571428571428571, 0.662, 0.666]\n",
      "Epoch 20: Loss 0.2060, Acc [0.9785714285714285, 0.764, 0.768]\n",
      "Epoch 30: Loss 0.1123, Acc [0.9928571428571429, 0.774, 0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Loss 0.0087, Acc [0.9928571428571429, 0.758, 0.737]\n",
      "Epoch 50: Loss 0.0053, Acc [0.9857142857142858, 0.754, 0.771]\n",
      "Epoch 60: Loss 0.0085, Acc [0.9928571428571429, 0.76, 0.731]\n",
      "Epoch 70: Loss 0.0657, Acc [0.9857142857142858, 0.766, 0.784]\n",
      "Epoch 80: Loss 0.0070, Acc [0.9857142857142858, 0.73, 0.743]\n",
      "Epoch 90: Loss 0.0103, Acc [0.9928571428571429, 0.758, 0.765]\n",
      "Total time: 2.79s\n",
      "\n",
      "--- [Cora] GIN | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.9552, Acc [0.15714285714285714, 0.178, 0.152]\n",
      "Epoch 10: Loss 0.4367, Acc [0.9571428571428572, 0.768, 0.754]\n",
      "Epoch 20: Loss 0.1049, Acc [0.9928571428571429, 0.736, 0.721]\n",
      "Epoch 30: Loss 0.0174, Acc [0.9857142857142858, 0.726, 0.731]\n",
      "Epoch 40: Loss 0.0047, Acc [0.9857142857142858, 0.706, 0.699]\n",
      "Epoch 50: Loss 0.0026, Acc [0.9928571428571429, 0.722, 0.723]\n",
      "Epoch 60: Loss 0.0477, Acc [0.9857142857142858, 0.764, 0.768]\n",
      "Epoch 70: Loss 0.0220, Acc [0.9857142857142858, 0.688, 0.697]\n",
      "Epoch 80: Loss 0.0193, Acc [0.9642857142857143, 0.702, 0.664]\n",
      "Epoch 90: Loss 0.0048, Acc [0.9928571428571429, 0.724, 0.739]\n",
      "Total time: 2.71s\n",
      "\n",
      "--- [Cora] GIN | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.9542, Acc [0.22142857142857142, 0.35, 0.362]\n",
      "Epoch 10: Loss 1.2611, Acc [0.8928571428571429, 0.722, 0.749]\n",
      "Epoch 20: Loss 0.2000, Acc [0.9785714285714285, 0.742, 0.757]\n",
      "Epoch 30: Loss 0.0307, Acc [0.9857142857142858, 0.704, 0.723]\n",
      "Epoch 40: Loss 0.0445, Acc [0.9857142857142858, 0.74, 0.759]\n",
      "Epoch 50: Loss 0.0062, Acc [0.9928571428571429, 0.728, 0.76]\n",
      "Epoch 60: Loss 0.0070, Acc [0.9857142857142858, 0.712, 0.745]\n",
      "Epoch 70: Loss 0.0080, Acc [0.9785714285714285, 0.74, 0.758]\n",
      "Epoch 80: Loss 0.0074, Acc [0.9857142857142858, 0.712, 0.74]\n",
      "Epoch 90: Loss 0.0120, Acc [0.9857142857142858, 0.748, 0.754]\n",
      "Total time: 2.62s\n",
      "\n",
      "--- [Citeseer] GCN | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7915, Acc [0.3, 0.174, 0.158]\n",
      "Epoch 10: Loss 1.6972, Acc [0.8916666666666667, 0.54, 0.515]\n",
      "Epoch 20: Loss 1.5561, Acc [0.925, 0.638, 0.626]\n",
      "Epoch 30: Loss 1.3912, Acc [0.925, 0.654, 0.646]\n",
      "Epoch 40: Loss 1.2078, Acc [0.9333333333333333, 0.688, 0.686]\n",
      "Epoch 50: Loss 1.0260, Acc [0.9333333333333333, 0.704, 0.702]\n",
      "Epoch 60: Loss 0.8641, Acc [0.9333333333333333, 0.704, 0.704]\n",
      "Epoch 70: Loss 0.7376, Acc [0.9416666666666667, 0.71, 0.713]\n",
      "Epoch 80: Loss 0.6535, Acc [0.9666666666666667, 0.71, 0.713]\n",
      "Epoch 90: Loss 0.5740, Acc [0.975, 0.708, 0.717]\n",
      "Total time: 4.96s\n",
      "\n",
      "--- [Citeseer] GCN | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7916, Acc [0.475, 0.18, 0.251]\n",
      "Epoch 10: Loss 1.6723, Acc [0.95, 0.678, 0.681]\n",
      "Epoch 20: Loss 1.4972, Acc [0.9416666666666667, 0.688, 0.691]\n",
      "Epoch 30: Loss 1.2689, Acc [0.95, 0.692, 0.702]\n",
      "Epoch 40: Loss 1.0628, Acc [0.9583333333333334, 0.696, 0.698]\n",
      "Epoch 50: Loss 0.8634, Acc [0.9666666666666667, 0.69, 0.7]\n",
      "Epoch 60: Loss 0.7235, Acc [0.95, 0.694, 0.701]\n",
      "Epoch 70: Loss 0.6151, Acc [0.975, 0.684, 0.706]\n",
      "Epoch 80: Loss 0.5417, Acc [0.9583333333333334, 0.682, 0.693]\n",
      "Epoch 90: Loss 0.4507, Acc [0.975, 0.684, 0.698]\n",
      "Total time: 2.44s\n",
      "\n",
      "--- [Citeseer] GCN | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.7917, Acc [0.525, 0.424, 0.438]\n",
      "Epoch 10: Loss 1.5715, Acc [0.9166666666666666, 0.69, 0.681]\n",
      "Epoch 20: Loss 1.2294, Acc [0.9333333333333333, 0.682, 0.692]\n",
      "Epoch 30: Loss 0.8947, Acc [0.95, 0.694, 0.7]\n",
      "Epoch 40: Loss 0.6413, Acc [0.9583333333333334, 0.696, 0.703]\n",
      "Epoch 50: Loss 0.5008, Acc [0.975, 0.698, 0.71]\n",
      "Epoch 60: Loss 0.4314, Acc [0.975, 0.69, 0.699]\n",
      "Epoch 70: Loss 0.3961, Acc [0.9833333333333333, 0.696, 0.698]\n",
      "Epoch 80: Loss 0.3512, Acc [0.9833333333333333, 0.692, 0.691]\n",
      "Epoch 90: Loss 0.3035, Acc [1.0, 0.692, 0.683]\n",
      "Total time: 2.37s\n",
      "\n",
      "--- [Citeseer] GCN | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.7917, Acc [0.175, 0.176, 0.187]\n",
      "Epoch 10: Loss 1.6668, Acc [0.7583333333333333, 0.468, 0.465]\n",
      "Epoch 20: Loss 1.2788, Acc [0.9166666666666666, 0.648, 0.647]\n",
      "Epoch 30: Loss 0.7631, Acc [0.9416666666666667, 0.652, 0.647]\n",
      "Epoch 40: Loss 0.4032, Acc [0.9333333333333333, 0.65, 0.628]\n",
      "Epoch 50: Loss 0.2492, Acc [0.9666666666666667, 0.626, 0.611]\n",
      "Epoch 60: Loss 0.1719, Acc [0.9666666666666667, 0.63, 0.622]\n",
      "Epoch 70: Loss 0.1358, Acc [0.9666666666666667, 0.646, 0.647]\n",
      "Epoch 80: Loss 0.1050, Acc [0.975, 0.648, 0.636]\n",
      "Epoch 90: Loss 0.0935, Acc [0.9833333333333333, 0.636, 0.623]\n",
      "Total time: 2.56s\n",
      "\n",
      "--- [Citeseer] GAT | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7920, Acc [0.9083333333333333, 0.512, 0.499]\n",
      "Epoch 10: Loss 1.3868, Acc [0.9333333333333333, 0.708, 0.7]\n",
      "Epoch 20: Loss 0.8604, Acc [0.9333333333333333, 0.716, 0.705]\n",
      "Epoch 30: Loss 0.5239, Acc [0.9583333333333334, 0.704, 0.711]\n",
      "Epoch 40: Loss 0.3825, Acc [0.975, 0.696, 0.705]\n",
      "Epoch 50: Loss 0.2922, Acc [0.9916666666666667, 0.696, 0.714]\n",
      "Epoch 60: Loss 0.2342, Acc [0.9916666666666667, 0.696, 0.714]\n",
      "Epoch 70: Loss 0.2103, Acc [1.0, 0.684, 0.7]\n",
      "Epoch 80: Loss 0.1797, Acc [1.0, 0.686, 0.704]\n",
      "Epoch 90: Loss 0.1683, Acc [1.0, 0.684, 0.706]\n",
      "Total time: 32.94s\n",
      "\n",
      "--- [Citeseer] GAT | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7913, Acc [0.925, 0.644, 0.64]\n",
      "Epoch 10: Loss 1.3801, Acc [0.9333333333333333, 0.708, 0.705]\n",
      "Epoch 20: Loss 0.8393, Acc [0.9333333333333333, 0.696, 0.702]\n",
      "Epoch 30: Loss 0.5039, Acc [0.9666666666666667, 0.684, 0.687]\n",
      "Epoch 40: Loss 0.3560, Acc [0.9833333333333333, 0.678, 0.686]\n",
      "Epoch 50: Loss 0.2735, Acc [0.9916666666666667, 0.674, 0.689]\n",
      "Epoch 60: Loss 0.2374, Acc [1.0, 0.676, 0.687]\n",
      "Epoch 70: Loss 0.2075, Acc [1.0, 0.69, 0.694]\n",
      "Epoch 80: Loss 0.1816, Acc [1.0, 0.682, 0.693]\n",
      "Epoch 90: Loss 0.1782, Acc [1.0, 0.682, 0.695]\n",
      "Total time: 12.50s\n",
      "\n",
      "--- [Citeseer] GAT | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.7921, Acc [0.9166666666666666, 0.636, 0.629]\n",
      "Epoch 10: Loss 0.9057, Acc [0.9333333333333333, 0.71, 0.696]\n",
      "Epoch 20: Loss 0.4004, Acc [0.975, 0.702, 0.708]\n",
      "Epoch 30: Loss 0.2732, Acc [0.9916666666666667, 0.706, 0.703]\n",
      "Epoch 40: Loss 0.1932, Acc [0.9916666666666667, 0.696, 0.699]\n",
      "Epoch 50: Loss 0.1745, Acc [1.0, 0.698, 0.69]\n",
      "Epoch 60: Loss 0.1546, Acc [1.0, 0.71, 0.704]\n",
      "Epoch 70: Loss 0.1396, Acc [1.0, 0.704, 0.706]\n",
      "Epoch 80: Loss 0.1341, Acc [1.0, 0.692, 0.7]\n",
      "Epoch 90: Loss 0.1157, Acc [1.0, 0.702, 0.69]\n",
      "Total time: 12.26s\n",
      "\n",
      "--- [Citeseer] GAT | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.7922, Acc [0.8916666666666667, 0.584, 0.573]\n",
      "Epoch 10: Loss 1.3764, Acc [0.9333333333333333, 0.704, 0.698]\n",
      "Epoch 20: Loss 0.8436, Acc [0.9333333333333333, 0.714, 0.699]\n",
      "Epoch 30: Loss 0.5143, Acc [0.9583333333333334, 0.7, 0.694]\n",
      "Epoch 40: Loss 0.3711, Acc [0.9916666666666667, 0.698, 0.691]\n",
      "Epoch 50: Loss 0.2995, Acc [0.9916666666666667, 0.698, 0.684]\n",
      "Epoch 60: Loss 0.2466, Acc [0.9916666666666667, 0.708, 0.69]\n",
      "Epoch 70: Loss 0.2247, Acc [0.9916666666666667, 0.696, 0.687]\n",
      "Epoch 80: Loss 0.1953, Acc [0.9916666666666667, 0.704, 0.688]\n",
      "Epoch 90: Loss 0.1804, Acc [1.0, 0.694, 0.69]\n",
      "Total time: 12.22s\n",
      "\n",
      "--- [Citeseer] GraphSAGE | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7916, Acc [0.16666666666666666, 0.188, 0.169]\n",
      "Epoch 10: Loss 1.6443, Acc [0.9666666666666667, 0.482, 0.439]\n",
      "Epoch 20: Loss 1.3397, Acc [0.9833333333333333, 0.626, 0.591]\n",
      "Epoch 30: Loss 0.9233, Acc [0.9916666666666667, 0.666, 0.673]\n",
      "Epoch 40: Loss 0.5596, Acc [1.0, 0.686, 0.703]\n",
      "Epoch 50: Loss 0.3661, Acc [1.0, 0.696, 0.693]\n",
      "Epoch 60: Loss 0.2746, Acc [1.0, 0.68, 0.692]\n",
      "Epoch 70: Loss 0.2230, Acc [1.0, 0.686, 0.691]\n",
      "Epoch 80: Loss 0.1884, Acc [1.0, 0.676, 0.698]\n",
      "Epoch 90: Loss 0.1880, Acc [1.0, 0.688, 0.7]\n",
      "Total time: 14.85s\n",
      "\n",
      "--- [Citeseer] GraphSAGE | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7957, Acc [0.16666666666666666, 0.188, 0.169]\n",
      "Epoch 10: Loss 1.6162, Acc [0.975, 0.46, 0.432]\n",
      "Epoch 20: Loss 1.2608, Acc [0.9916666666666667, 0.65, 0.643]\n",
      "Epoch 30: Loss 0.8251, Acc [0.9916666666666667, 0.688, 0.686]\n",
      "Epoch 40: Loss 0.4882, Acc [1.0, 0.712, 0.704]\n",
      "Epoch 50: Loss 0.3260, Acc [1.0, 0.688, 0.693]\n",
      "Epoch 60: Loss 0.2600, Acc [1.0, 0.692, 0.703]\n",
      "Epoch 70: Loss 0.2443, Acc [1.0, 0.692, 0.696]\n",
      "Epoch 80: Loss 0.2006, Acc [1.0, 0.696, 0.703]\n",
      "Epoch 90: Loss 0.1615, Acc [1.0, 0.698, 0.703]\n",
      "Total time: 5.28s\n",
      "\n",
      "--- [Citeseer] GraphSAGE | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.7949, Acc [0.30833333333333335, 0.19, 0.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss 1.3291, Acc [1.0, 0.632, 0.645]\n",
      "Epoch 20: Loss 0.5994, Acc [1.0, 0.688, 0.68]\n",
      "Epoch 30: Loss 0.2674, Acc [1.0, 0.674, 0.687]\n",
      "Epoch 40: Loss 0.2070, Acc [1.0, 0.676, 0.685]\n",
      "Epoch 50: Loss 0.1704, Acc [1.0, 0.686, 0.694]\n",
      "Epoch 60: Loss 0.1606, Acc [1.0, 0.692, 0.687]\n",
      "Epoch 70: Loss 0.1252, Acc [1.0, 0.702, 0.695]\n",
      "Epoch 80: Loss 0.1243, Acc [1.0, 0.694, 0.684]\n",
      "Epoch 90: Loss 0.1356, Acc [1.0, 0.692, 0.694]\n",
      "Total time: 5.33s\n",
      "\n",
      "--- [Citeseer] GraphSAGE | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.7950, Acc [0.475, 0.22, 0.213]\n",
      "Epoch 10: Loss 1.6227, Acc [1.0, 0.586, 0.589]\n",
      "Epoch 20: Loss 1.2804, Acc [1.0, 0.668, 0.654]\n",
      "Epoch 30: Loss 0.8529, Acc [1.0, 0.708, 0.682]\n",
      "Epoch 40: Loss 0.4956, Acc [1.0, 0.702, 0.692]\n",
      "Epoch 50: Loss 0.3284, Acc [1.0, 0.686, 0.687]\n",
      "Epoch 60: Loss 0.2524, Acc [1.0, 0.688, 0.695]\n",
      "Epoch 70: Loss 0.2382, Acc [1.0, 0.688, 0.701]\n",
      "Epoch 80: Loss 0.2301, Acc [1.0, 0.696, 0.702]\n",
      "Epoch 90: Loss 0.2162, Acc [1.0, 0.694, 0.703]\n",
      "Total time: 5.11s\n",
      "\n",
      "--- [Citeseer] GIN | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.8016, Acc [0.21666666666666667, 0.206, 0.213]\n",
      "Epoch 10: Loss 1.4383, Acc [0.625, 0.536, 0.529]\n",
      "Epoch 20: Loss 0.5673, Acc [0.9583333333333334, 0.532, 0.527]\n",
      "Epoch 30: Loss 0.1268, Acc [1.0, 0.558, 0.575]\n",
      "Epoch 40: Loss 0.0234, Acc [1.0, 0.52, 0.532]\n",
      "Epoch 50: Loss 0.0440, Acc [1.0, 0.598, 0.596]\n",
      "Epoch 60: Loss 0.0152, Acc [1.0, 0.572, 0.588]\n",
      "Epoch 70: Loss 0.0710, Acc [1.0, 0.566, 0.566]\n",
      "Epoch 80: Loss 0.0234, Acc [1.0, 0.604, 0.616]\n",
      "Epoch 90: Loss 0.0134, Acc [1.0, 0.602, 0.605]\n",
      "Total time: 11.86s\n",
      "\n",
      "--- [Citeseer] GIN | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7915, Acc [0.16666666666666666, 0.232, 0.181]\n",
      "Epoch 10: Loss 1.3502, Acc [0.7666666666666667, 0.582, 0.536]\n",
      "Epoch 20: Loss 0.3669, Acc [0.975, 0.602, 0.582]\n",
      "Epoch 30: Loss 0.0752, Acc [1.0, 0.584, 0.564]\n",
      "Epoch 40: Loss 0.0423, Acc [0.9916666666666667, 0.606, 0.628]\n",
      "Epoch 50: Loss 0.0429, Acc [1.0, 0.598, 0.599]\n",
      "Epoch 60: Loss 0.0189, Acc [1.0, 0.604, 0.606]\n",
      "Epoch 70: Loss 0.0216, Acc [1.0, 0.62, 0.617]\n",
      "Epoch 80: Loss 0.0270, Acc [0.9916666666666667, 0.6, 0.598]\n",
      "Epoch 90: Loss 0.0264, Acc [1.0, 0.596, 0.58]\n",
      "Total time: 4.32s\n",
      "\n",
      "--- [Citeseer] GIN | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.7996, Acc [0.18333333333333332, 0.238, 0.184]\n",
      "Epoch 10: Loss 0.7692, Acc [0.8916666666666667, 0.464, 0.464]\n",
      "Epoch 20: Loss 0.1006, Acc [1.0, 0.574, 0.574]\n",
      "Epoch 30: Loss 0.0635, Acc [0.9916666666666667, 0.592, 0.617]\n",
      "Epoch 40: Loss 0.0128, Acc [1.0, 0.612, 0.614]\n",
      "Epoch 50: Loss 0.0118, Acc [0.9916666666666667, 0.638, 0.64]\n",
      "Epoch 60: Loss 0.0245, Acc [1.0, 0.614, 0.606]\n",
      "Epoch 70: Loss 0.0116, Acc [1.0, 0.616, 0.632]\n",
      "Epoch 80: Loss 0.0040, Acc [1.0, 0.618, 0.624]\n",
      "Epoch 90: Loss 0.0030, Acc [1.0, 0.636, 0.643]\n",
      "Total time: 4.41s\n",
      "\n",
      "--- [Citeseer] GIN | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.7975, Acc [0.16666666666666666, 0.194, 0.176]\n",
      "Epoch 10: Loss 1.3480, Acc [0.8, 0.506, 0.486]\n",
      "Epoch 20: Loss 0.3121, Acc [0.9833333333333333, 0.624, 0.624]\n",
      "Epoch 30: Loss 0.0939, Acc [1.0, 0.62, 0.644]\n",
      "Epoch 40: Loss 0.1330, Acc [0.9916666666666667, 0.598, 0.593]\n",
      "Epoch 50: Loss 0.0459, Acc [1.0, 0.636, 0.631]\n",
      "Epoch 60: Loss 0.0067, Acc [1.0, 0.62, 0.62]\n",
      "Epoch 70: Loss 0.0141, Acc [1.0, 0.616, 0.618]\n",
      "Epoch 80: Loss 0.0092, Acc [1.0, 0.63, 0.629]\n",
      "Epoch 90: Loss 0.0109, Acc [1.0, 0.632, 0.626]\n",
      "Total time: 4.40s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://drive.usercontent.google.com/download?id=1crmsTbd1-2sEXsGwa2IKnIB7Zd3TmUsy&confirm=t\n",
      "Downloading https://drive.usercontent.google.com/download?id=1join-XdvX3anJU_MLVtick7MgeAQiWIZ&confirm=t\n",
      "Downloading https://drive.usercontent.google.com/download?id=1uxIkbtg5drHTsKt-PAsZZ4_yJmgFmle9&confirm=t\n",
      "Downloading https://drive.usercontent.google.com/download?id=1htXCtuktuCW8TR8KiKfrFDAxUgekQoV7&confirm=t\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Flickr] GCN | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9454, Acc [0.2614453781512605, 0.2603531731803514, 0.2578317572715457]\n",
      "Epoch 10: Loss 1.7412, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 20: Loss 1.6368, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 30: Loss 1.6318, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 40: Loss 1.6308, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 50: Loss 1.6273, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 60: Loss 1.6229, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 70: Loss 1.6218, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 80: Loss 1.6180, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 90: Loss 1.6141, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Total time: 76.47s\n",
      "\n",
      "--- [Flickr] GCN | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7015, Acc [0.42162464985994397, 0.42389745428468983, 0.4234302872764756]\n",
      "Epoch 10: Loss 1.5764, Acc [0.42337254901960786, 0.42528684116170673, 0.4252677811141487]\n",
      "Epoch 20: Loss 1.5676, Acc [0.4241344537815126, 0.42663140910720687, 0.42647783803164074]\n",
      "Epoch 30: Loss 1.5647, Acc [0.4246274509803922, 0.4272588741484403, 0.4272845426433021]\n",
      "Epoch 40: Loss 1.5649, Acc [0.4250980392156863, 0.4277518823951237, 0.4274638103347824]\n",
      "Epoch 50: Loss 1.5627, Acc [0.4243137254901961, 0.42703477949085694, 0.4269260072603415]\n",
      "Epoch 60: Loss 1.5627, Acc [0.42451540616246497, 0.427169236285407, 0.42701564110608164]\n",
      "Epoch 70: Loss 1.5620, Acc [0.4246274509803922, 0.4272140552169236, 0.4272845426433021]\n",
      "Epoch 80: Loss 1.5624, Acc [0.4251204481792717, 0.4277518823951237, 0.42755344418052255]\n",
      "Epoch 90: Loss 1.5622, Acc [0.42581512605042016, 0.4284689852993905, 0.4285394164836642]\n",
      "Total time: 1162.14s\n",
      "\n",
      "--- [Flickr] GCN | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.6701, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 10: Loss 1.5678, Acc [0.42532212885154064, 0.427841520258157, 0.42809124725496345]\n",
      "Epoch 20: Loss 1.5624, Acc [0.4266218487394958, 0.4287378988884905, 0.4291668534038453]\n",
      "Epoch 30: Loss 1.5621, Acc [0.4255686274509804, 0.4280207959842237, 0.42831533186931386]\n",
      "Epoch 40: Loss 1.5624, Acc [0.4276302521008403, 0.4299928289709573, 0.43064581185855777]\n",
      "Epoch 50: Loss 1.5619, Acc [0.4241120448179272, 0.42663140910720687, 0.4263882041859006]\n",
      "Epoch 60: Loss 1.5622, Acc [0.42657703081232495, 0.42878271782000715, 0.428987585712365]\n",
      "Epoch 70: Loss 1.5612, Acc [0.42550140056022406, 0.4280207959842237, 0.4282256980235737]\n",
      "Epoch 80: Loss 1.5615, Acc [0.4245826330532213, 0.4273933309429903, 0.4271500918746919]\n",
      "Epoch 90: Loss 1.5615, Acc [0.4244705882352941, 0.4272140552169236, 0.42701564110608164]\n",
      "Total time: 1066.69s\n",
      "\n",
      "--- [Flickr] GCN | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.6948, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 10: Loss 1.5566, Acc [0.431327731092437, 0.43402653280745784, 0.4339622641509434]\n",
      "Epoch 20: Loss 1.5526, Acc [0.43746778711484596, 0.4390462531373252, 0.43938511181822254]\n",
      "Epoch 30: Loss 1.5504, Acc [0.4319775910364146, 0.4342954463965579, 0.4345000672253843]\n",
      "Epoch 40: Loss 1.5524, Acc [0.4282801120448179, 0.4306651129437074, 0.431362882624479]\n",
      "Epoch 50: Loss 1.5514, Acc [0.4298487394957983, 0.4321889566152743, 0.4333796441536324]\n",
      "Epoch 60: Loss 1.5511, Acc [0.42550140056022406, 0.42752778773754035, 0.42773271187200285]\n",
      "Epoch 70: Loss 1.5511, Acc [0.4295574229691877, 0.4316959483685909, 0.43270739031058125]\n",
      "Epoch 80: Loss 1.5522, Acc [0.42626330532212886, 0.4284689852993905, 0.42849459956079416]\n",
      "Epoch 90: Loss 1.5516, Acc [0.426890756302521, 0.4291412692721406, 0.42939093801819567]\n",
      "Total time: 1298.96s\n",
      "\n",
      "--- [Flickr] GAT | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9477, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 10: Loss 1.6041, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 20: Loss 1.5860, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 30: Loss 1.5781, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 40: Loss 1.5755, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 50: Loss 1.5735, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 60: Loss 1.5725, Acc [0.42160224089635856, 0.42389745428468983, 0.4234302872764756]\n",
      "Epoch 70: Loss 1.5698, Acc [0.42178151260504204, 0.4239870921477232, 0.42351992112221576]\n",
      "Epoch 80: Loss 1.5670, Acc [0.42238655462184876, 0.42461455718895663, 0.42423699188813696]\n",
      "Epoch 90: Loss 1.5618, Acc [0.4238655462184874, 0.42663140910720687, 0.4258952180343298]\n",
      "Total time: 679.36s\n",
      "\n",
      "--- [Flickr] GAT | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.6215, Acc [0.42180392156862745, 0.4239870921477232, 0.4235647380450858]\n",
      "Epoch 10: Loss 1.5620, Acc [0.42527731092436977, 0.42770706346360704, 0.42741899341191236]\n",
      "Epoch 20: Loss 1.5613, Acc [0.42388795518207284, 0.42613840086052346, 0.42594003495719984]\n",
      "Epoch 30: Loss 1.5609, Acc [0.4355182072829132, 0.43801541771244173, 0.4373683502890692]\n",
      "Epoch 40: Loss 1.5610, Acc [0.44136694677871147, 0.4452760846181427, 0.44382198718236004]\n",
      "Epoch 50: Loss 1.5608, Acc [0.42633053221288514, 0.42882753675152385, 0.4281808811007036]\n",
      "Epoch 60: Loss 1.5618, Acc [0.42417927170868347, 0.4264521333811402, 0.4261641195715502]\n",
      "Epoch 70: Loss 1.5601, Acc [0.42467226890756304, 0.42681068483327356, 0.4265226549545108]\n",
      "Epoch 80: Loss 1.5606, Acc [0.4307002801120448, 0.4333542488347078, 0.43297629184780173]\n",
      "Epoch 90: Loss 1.5601, Acc [0.42245378151260504, 0.4243456435998566, 0.42387845650517636]\n",
      "Total time: 5733.63s\n",
      "\n",
      "--- [Flickr] GAT | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.6121, Acc [0.42178151260504204, 0.42394227321620653, 0.42351992112221576]\n",
      "Epoch 10: Loss 1.5608, Acc [0.4595854341736695, 0.4608282538544281, 0.4601353471070676]\n",
      "Epoch 20: Loss 1.5625, Acc [0.4250532212885154, 0.427438149874507, 0.4271500918746919]\n",
      "Epoch 30: Loss 1.5610, Acc [0.4326498599439776, 0.43496773036930797, 0.434320799533904]\n",
      "Epoch 40: Loss 1.5609, Acc [0.4547450980392157, 0.457556471853711, 0.4564603594317214]\n",
      "Epoch 50: Loss 1.5613, Acc [0.4336358543417367, 0.4359089279311581, 0.43571012414287635]\n",
      "Epoch 60: Loss 1.5620, Acc [0.425187675070028, 0.42770706346360704, 0.42737417648904225]\n",
      "Epoch 70: Loss 1.5614, Acc [0.4247170868347339, 0.42703477949085694, 0.4263882041859006]\n",
      "Epoch 80: Loss 1.5610, Acc [0.43105882352941177, 0.43357834349229113, 0.43315555953928203]\n",
      "Epoch 90: Loss 1.5613, Acc [0.4242913165266106, 0.4264521333811402, 0.42647783803164074]\n",
      "Total time: 5531.63s\n",
      "\n",
      "--- [Flickr] GAT | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.6215, Acc [0.42178151260504204, 0.4239870921477232, 0.4235647380450858]\n",
      "Epoch 10: Loss 1.5618, Acc [0.42451540616246497, 0.4267210469702402, 0.4265226549545108]\n",
      "Epoch 20: Loss 1.5609, Acc [0.4315518207282913, 0.43407135173897454, 0.43329001030789227]\n",
      "Epoch 30: Loss 1.5606, Acc [0.4249187675070028, 0.42748296880602366, 0.42683637341460134]\n",
      "Epoch 40: Loss 1.5604, Acc [0.42503081232492995, 0.4273933309429903, 0.4271052749518218]\n",
      "Epoch 50: Loss 1.5599, Acc [0.425187675070028, 0.4276174256005737, 0.42741899341191236]\n",
      "Epoch 60: Loss 1.5608, Acc [0.42792156862745095, 0.4303961993546074, 0.42974947340115627]\n",
      "Epoch 70: Loss 1.5612, Acc [0.42561344537815127, 0.4278863391896737, 0.42755344418052255]\n",
      "Epoch 80: Loss 1.5598, Acc [0.422812324929972, 0.4246593761204733, 0.4244610765024873]\n",
      "Epoch 90: Loss 1.5612, Acc [0.423484593837535, 0.42524202223019003, 0.42531259803701876]\n",
      "Total time: 5335.95s\n",
      "\n",
      "--- [Flickr] GraphSAGE | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9715, Acc [0.05447619047619048, 0.054903191107923986, 0.055886702818984446]\n",
      "Epoch 10: Loss 1.6387, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 20: Loss 1.6022, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 30: Loss 1.5914, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 40: Loss 1.5824, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Loss 1.5800, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 60: Loss 1.5753, Acc [0.42160224089635856, 0.42394227321620653, 0.4234302872764756]\n",
      "Epoch 70: Loss 1.5708, Acc [0.4230812324929972, 0.4253764790247401, 0.42432662573387714]\n",
      "Epoch 80: Loss 1.5645, Acc [0.42738375350140057, 0.43093402653280743, 0.43001837493837675]\n",
      "Epoch 90: Loss 1.5603, Acc [0.43318767507002803, 0.4355951954105414, 0.4353964056827858]\n",
      "Total time: 161.44s\n",
      "\n",
      "--- [Flickr] GraphSAGE | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.7008, Acc [0.42160224089635856, 0.4238526353531732, 0.4234302872764756]\n",
      "Epoch 10: Loss 1.5714, Acc [0.46384313725490195, 0.4606489781283614, 0.4618383901761305]\n",
      "Epoch 20: Loss 1.5607, Acc [0.46525490196078434, 0.4617246324847616, 0.46304844709362253]\n",
      "Epoch 30: Loss 1.5598, Acc [0.46639775910364145, 0.46401039799211186, 0.4659167301573074]\n",
      "Epoch 40: Loss 1.5566, Acc [0.4666666666666667, 0.46360702760846184, 0.465110025545646]\n",
      "Epoch 50: Loss 1.5551, Acc [0.4666666666666667, 0.46383112226604517, 0.4653789270828665]\n",
      "Epoch 60: Loss 1.5518, Acc [0.46408963585434176, 0.4619487271423449, 0.4634069824765832]\n",
      "Epoch 70: Loss 1.5513, Acc [0.46619607843137256, 0.46356220867694514, 0.4646618563169453]\n",
      "Epoch 80: Loss 1.5506, Acc [0.4647394957983193, 0.46315883829329507, 0.46381033478241385]\n",
      "Epoch 90: Loss 1.5504, Acc [0.46431372549019606, 0.462352097525995, 0.4629588132478824]\n",
      "Total time: 1660.37s\n",
      "\n",
      "--- [Flickr] GraphSAGE | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.6507, Acc [0.4252997198879552, 0.427841520258157, 0.42719490879756195]\n",
      "Epoch 10: Loss 1.5602, Acc [0.46252100840336136, 0.4593044101828612, 0.46156948863891006]\n",
      "Epoch 20: Loss 1.5553, Acc [0.46494117647058825, 0.4619039082108283, 0.46255546094205174]\n",
      "Epoch 30: Loss 1.5535, Acc [0.4659047619047619, 0.4619039082108283, 0.4642136870882445]\n",
      "Epoch 40: Loss 1.5519, Acc [0.4666666666666667, 0.4659376120473288, 0.4657374624658271]\n",
      "Epoch 50: Loss 1.5514, Acc [0.466890756302521, 0.46351738974542844, 0.46569264554295703]\n",
      "Epoch 60: Loss 1.5508, Acc [0.4670028011204482, 0.4645482251703119, 0.4664545332317483]\n",
      "Epoch 70: Loss 1.5517, Acc [0.46724929971988793, 0.46468268196486195, 0.4671716039976695]\n",
      "Epoch 80: Loss 1.5511, Acc [0.46718207282913166, 0.46620652563642884, 0.4664545332317483]\n",
      "Epoch 90: Loss 1.5504, Acc [0.46673389355742295, 0.46620652563642884, 0.46623044861739793]\n",
      "Total time: 1755.30s\n",
      "\n",
      "--- [Flickr] GraphSAGE | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.6945, Acc [0.42160224089635856, 0.42394227321620653, 0.4234302872764756]\n",
      "Epoch 10: Loss 1.5706, Acc [0.46377591036414567, 0.46006633201864466, 0.46134540402455965]\n",
      "Epoch 20: Loss 1.5614, Acc [0.46619607843137256, 0.4629795625672284, 0.4643033209339847]\n",
      "Epoch 30: Loss 1.5577, Acc [0.4660168067226891, 0.46333811401936176, 0.4653789270828665]\n",
      "Epoch 40: Loss 1.5552, Acc [0.46698039215686277, 0.46418967371817854, 0.46560301169721685]\n",
      "Epoch 50: Loss 1.5538, Acc [0.4642016806722689, 0.46154535675869485, 0.46309326401649265]\n",
      "Epoch 60: Loss 1.5526, Acc [0.46588235294117647, 0.4621728217999283, 0.4639447855510241]\n",
      "Epoch 70: Loss 1.5518, Acc [0.46592717086834734, 0.46338293295087846, 0.46524447631425625]\n",
      "Epoch 80: Loss 1.5507, Acc [0.46657703081232493, 0.46356220867694514, 0.4660959978487877]\n",
      "Epoch 90: Loss 1.5512, Acc [0.46704761904761904, 0.46405521692362856, 0.4668130686147089]\n",
      "Total time: 1850.57s\n",
      "\n",
      "--- [Flickr] GIN | Sampler = False | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.9990, Acc [0.304, 0.30530656149157404, 0.29915296015775555]\n",
      "Epoch 10: Loss 1.7700, Acc [0.3047394957983193, 0.30266224453209034, 0.29700174785999195]\n",
      "Epoch 20: Loss 1.6939, Acc [0.493266106442577, 0.4912154894227322, 0.4925827992650025]\n",
      "Epoch 30: Loss 1.6300, Acc [0.4946778711484594, 0.4929186088203657, 0.4978263792408013]\n",
      "Epoch 40: Loss 1.5671, Acc [0.4789467787114846, 0.4806830405163141, 0.4832608793080267]\n",
      "Epoch 50: Loss 1.5181, Acc [0.47520448179271707, 0.477276801721047, 0.47998924393851117]\n",
      "Epoch 60: Loss 1.4966, Acc [0.47489075630252103, 0.47588741484403013, 0.47918253932684984]\n",
      "Epoch 70: Loss 1.4911, Acc [0.5010644257703081, 0.5008963786303334, 0.5034733115224309]\n",
      "Epoch 80: Loss 1.4840, Acc [0.5061288515406163, 0.5050645392613841, 0.5074620176578676]\n",
      "Epoch 90: Loss 1.4906, Acc [0.4951932773109244, 0.4951147364646827, 0.4951373638685968]\n",
      "Total time: 158.67s\n",
      "\n",
      "--- [Flickr] GIN | Sampler = True | LR=0.005, Layers=2 ---\n",
      "Epoch 0: Loss 1.6667, Acc [0.4231484593837535, 0.4244801003944066, 0.42477479496257786]\n",
      "Epoch 10: Loss 1.5256, Acc [0.4380728291316527, 0.4365812119039082, 0.4370994487518487]\n",
      "Epoch 20: Loss 1.4868, Acc [0.5003697478991597, 0.4962800286841162, 0.49554071617442746]\n",
      "Epoch 30: Loss 1.4813, Acc [0.49799439775910365, 0.4961007529580495, 0.4948684623313763]\n",
      "Epoch 40: Loss 1.4773, Acc [0.5089299719887955, 0.5071710290426676, 0.503562945368171]\n",
      "Epoch 50: Loss 1.4742, Acc [0.48985994397759103, 0.4889745428468985, 0.48720476852059336]\n",
      "Epoch 60: Loss 1.4742, Acc [0.5011764705882353, 0.4990139835066332, 0.49679559001478957]\n",
      "Epoch 70: Loss 1.4702, Acc [0.49768067226890755, 0.4962800286841162, 0.4954958992515574]\n",
      "Epoch 80: Loss 1.4654, Acc [0.50296918767507, 0.49986554320544996, 0.4957648007887778]\n",
      "Epoch 90: Loss 1.4632, Acc [0.49080112044817925, 0.48758515596988167, 0.4859947116031013]\n",
      "Total time: 1617.97s\n",
      "\n",
      "--- [Flickr] GIN | Sampler = True | LR=0.01, Layers=2 ---\n",
      "Epoch 0: Loss 1.6511, Acc [0.4213109243697479, 0.4236733596271065, 0.4232510195849953]\n",
      "Epoch 10: Loss 1.5360, Acc [0.46579271708683473, 0.4653997848691287, 0.46878501322099225]\n",
      "Epoch 20: Loss 1.4960, Acc [0.46496358543417365, 0.462621011115095, 0.4629588132478824]\n",
      "Epoch 30: Loss 1.4903, Acc [0.4750028011204482, 0.47257081391179634, 0.47420786088827144]\n",
      "Epoch 40: Loss 1.4925, Acc [0.46850420168067225, 0.46616170670491214, 0.4681575763008112]\n",
      "Epoch 50: Loss 1.4887, Acc [0.47585434173669466, 0.47418429544639656, 0.47420786088827144]\n",
      "Epoch 60: Loss 1.4868, Acc [0.4746218487394958, 0.4705091430620294, 0.4731770716622597]\n",
      "Epoch 70: Loss 1.4853, Acc [0.467563025210084, 0.4619935460738616, 0.46564782862008697]\n",
      "Epoch 80: Loss 1.4849, Acc [0.4750924369747899, 0.47261563284331304, 0.4738941424281809]\n",
      "Epoch 90: Loss 1.4838, Acc [0.45467787114845937, 0.44908569379705987, 0.4513064133016627]\n",
      "Total time: 1676.82s\n",
      "\n",
      "--- [Flickr] GIN | Sampler = True | LR=0.005, Layers=3 ---\n",
      "Epoch 0: Loss 1.6917, Acc [0.4238655462184874, 0.4246593761204733, 0.426612288800251]\n",
      "Epoch 10: Loss 1.5187, Acc [0.48324929971988795, 0.4805037647902474, 0.48223009008201495]\n",
      "Epoch 20: Loss 1.4893, Acc [0.40479551820728293, 0.40005378271782, 0.3979742750862726]\n",
      "Epoch 30: Loss 1.4888, Acc [0.3858151260504202, 0.3819021154535676, 0.37968897055528167]\n",
      "Epoch 40: Loss 1.4840, Acc [0.4088963585434174, 0.40507350304768736, 0.40043920584412673]\n",
      "Epoch 50: Loss 1.4801, Acc [0.468078431372549, 0.46445858730727857, 0.46282436247927217]\n",
      "Epoch 60: Loss 1.4790, Acc [0.4775126050420168, 0.4739602007888132, 0.469815802447004]\n",
      "Epoch 70: Loss 1.4770, Acc [0.4602577030812325, 0.4551362495518107, 0.453278357907946]\n",
      "Epoch 80: Loss 1.4741, Acc [0.5059271708683474, 0.502465041233417, 0.5019047192219782]\n",
      "Epoch 90: Loss 1.4714, Acc [0.4913389355742297, 0.48709214772319825, 0.4855017254515305]\n",
      "Total time: 1666.48s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run(dataset_name, model_name, use_sampler=False, epochs=100, lr=0.005, layers=2):\n",
    "    # 加载数据集和初始化模型\n",
    "    dataset = load_data(dataset_name)\n",
    "    data = dataset[0].to(device)  # 将图数据转移到设备（GPU或CPU）\n",
    "    # 获取输入维度（节点特征维度）和输出维度（类别数）\n",
    "    in_dim = dataset.num_node_features\n",
    "    out_dim = dataset.num_classes\n",
    "    # 用一个字典映射模型名称到对应类，简洁灵活\n",
    "    model_cls = {'GCN': GCN, 'GAT': GAT, 'GraphSAGE': GraphSAGE, 'GIN': GIN}[model_name]\n",
    "    # 特殊处理：GCN 支持自定义层数（num_layers），其他模型没有这个参数\n",
    "    if model_name == 'GCN':\n",
    "        model = model_cls(in_dim, 64, out_dim, num_layers=layers).to(device)\n",
    "    else:\n",
    "        model = model_cls(in_dim, 64, out_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "     # 打印当前实验配置信息\n",
    "    print(f\"--- [{dataset_name}] {model_name} | Sampler = {use_sampler} | LR={lr}, Layers={layers} ---\")\n",
    "\n",
    "    '''模型训练部分'''\n",
    "    # 启动计时器\n",
    "    start = time.time()\n",
    "    # 子图采样训练模式\n",
    "    if use_sampler:\n",
    "        loader = NeighborLoader(\n",
    "            data,   # 完整图数据\n",
    "            input_nodes=data.train_mask,    # 指定训练节点\n",
    "            num_neighbors=[15, 10],    # 每层采样邻居数\n",
    "            batch_size=1024\n",
    "        )\n",
    "        for epoch in range(epochs):\n",
    "            loss = train_sample(model, loader, optimizer)\n",
    "            if epoch % 10 == 0:    # 每10轮打印一次精度\n",
    "                accs = test(model, data)\n",
    "                print(f\"Epoch {epoch}: Loss {loss:.4f}, Acc {accs}\")\n",
    "    # 全图训练模式\n",
    "    else:\n",
    "        for epoch in range(epochs):\n",
    "            loss = train_full(model, data, optimizer)\n",
    "            if epoch % 10 == 0:\n",
    "                accs = test(model, data)\n",
    "                print(f\"Epoch {epoch}: Loss {loss:.4f}, Acc {accs}\")\n",
    "    print(f\"Total time: {time.time() - start:.2f}s\\n\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    datasets = ['Cora', 'Citeseer', 'Flickr']\n",
    "    models = ['GCN', 'GAT', 'GraphSAGE', 'GIN']\n",
    "    for d in datasets:\n",
    "        for m in models:\n",
    "            run(d, m, use_sampler=False)\n",
    "            run(d, m, use_sampler=True)\n",
    "            run(d, m, use_sampler=True, lr=0.01)\n",
    "            run(d, m, use_sampler=True, layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4e04a",
   "metadata": {},
   "source": [
    "# 图上的链路预测\n",
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e645790",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''标准图卷积：聚合邻居节点特征'''\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        return self.convs[-1](x, edge_index)\n",
    "'''图注意力卷积：适合处理图中连接权重不均的情况'''\n",
    "class GATEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=4, concat=True))\n",
    "        for _ in range(layers - 2):\n",
    "            self.convs.append(GATConv(hidden_channels * 4, hidden_channels, heads=4, concat=True))\n",
    "        self.convs.append(GATConv(hidden_channels * 4, hidden_channels, heads=1, concat=True))\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = F.elu(conv(x, edge_index))\n",
    "        return self.convs[-1](x, edge_index)\n",
    "'''邻居聚合 + 自身连接：支持大图的采样训练，适用于归纳学习'''\n",
    "class SAGEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        return self.convs[-1](x, edge_index)\n",
    "'''图同构网络'''\n",
    "class GINEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(layers):\n",
    "            in_dim = in_channels if i == 0 else hidden_channels\n",
    "            mlp = Sequential(Linear(in_dim, hidden_channels), ReLU(), Linear(hidden_channels, hidden_channels))\n",
    "            self.convs.append(GINConv(mlp))\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        return self.convs[-1](x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923650ae",
   "metadata": {},
   "source": [
    "## 链路预测（解码器）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00781551",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''解码器'''\n",
    "def decode(z, edge_index):\n",
    "    # Inner product decoder\n",
    "    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
    "'''构造训练目标标签'''\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # 正样本边（图中真实存在的边）\n",
    "    pos_labels = torch.ones(pos_edge_index.size(1))\n",
    "    # 负样本边（图中不存在的边，通过负采样得到）\n",
    "    neg_labels = torch.zeros(neg_edge_index.size(1))\n",
    "    return torch.cat([pos_labels, neg_labels], dim=0)\n",
    "'''评估函数'''\n",
    "def evaluate(model, data, pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.edge_index)\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "        pred = decode(z, edge_index).sigmoid()\n",
    "        labels = get_link_labels(pos_edge_index, neg_edge_index).to(pred.device)\n",
    "        auc = roc_auc_score(labels.cpu(), pred.cpu())\n",
    "        ap = average_precision_score(labels.cpu(), pred.cpu())\n",
    "        return auc, ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99794c44",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a508b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalOnlyPlanetoid(Planetoid):\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        if self.name.lower() == 'cora':\n",
    "            return ['cora.content', 'cora.cites']\n",
    "        elif self.name.lower() == 'citeseer':\n",
    "            return ['citeseer.content', 'citeseer.cites']\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported Planetoid dataset: {self.name}\")\n",
    "    def download(self):\n",
    "        print(f\"[INFO] 使用本地文件加载数据集：{self.name}\")\n",
    "        print(f\"检查目录：{self.raw_dir}\")\n",
    "        print(f\"期望文件：{self.raw_file_names}\")\n",
    "        missing = [f for f in self.raw_paths if not os.path.exists(f)]\n",
    "        if missing:\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] 缺少文件: {missing}\\n\"\n",
    "                f\"请手动下载并放置到目录：{self.raw_dir}\\n\"\n",
    "                f\"Cora/Citeseer 下载地址：https://linqs-data.soe.ucsc.edu/public/lbc/\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"[OK] 所有原始文件已找到，开始加载。\")\n",
    "\n",
    "# ========== 通用加载函数 ==========\n",
    "def load_data(name):\n",
    "    name = name.lower()\n",
    "    base_path = r'D:\\Data\\master\\Graph Machine Learning\\GNN\\standard benchmark datasets'\n",
    "    if name in ['cora', 'citeseer']:\n",
    "        dataset = LocalOnlyPlanetoid(\n",
    "            root=os.path.join(base_path),\n",
    "            name=name,\n",
    "            transform=NormalizeFeatures()\n",
    "        )\n",
    "        data = dataset[0]\n",
    "        data.edge_index = ToUndirected()(data).edge_index\n",
    "        data = train_test_split_edges(data)  \n",
    "        return data, dataset.num_node_features\n",
    "    elif name == 'flickr':\n",
    "        dataset = Flickr(\n",
    "            root=os.path.join(base_path, 'Flickr'),\n",
    "            transform=NormalizeFeatures()\n",
    "        )\n",
    "        data = dataset[0]\n",
    "        data.edge_index = ToUndirected()(data).edge_index\n",
    "        # 对Flickr用采样器训练，不做 train_test_split_edges\n",
    "        return data, dataset.num_node_features\n",
    "    else:\n",
    "        raise ValueError(\"Only 'cora', 'citeseer', and 'flickr' datasets are supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3636a",
   "metadata": {},
   "source": [
    "## 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7ec5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, device):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if hasattr(data, 'train_pos_edge_index'):\n",
    "        # 链路预测任务\n",
    "        z = model(data.x, data.train_pos_edge_index)\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=data.train_pos_edge_index,\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=data.train_pos_edge_index.size(1)\n",
    "        )\n",
    "        edge_index = torch.cat([data.train_pos_edge_index, neg_edge_index], dim=-1)\n",
    "        labels = get_link_labels(data.train_pos_edge_index, neg_edge_index).to(device)\n",
    "        pred = decode(z, edge_index).sigmoid()\n",
    "        loss = F.binary_cross_entropy(pred, labels)\n",
    "    else:\n",
    "        # 非链路预测任务\n",
    "        z = model(data.x, data.edge_index)\n",
    "        loss = z.norm(p=2).mean()  \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.no_grad()\n",
    "def test(model, data, device):\n",
    "    model.eval()\n",
    "    if not hasattr(data, 'val_pos_edge_index'):\n",
    "        print(\"No link prediction evaluation for this dataset.\")\n",
    "        return None, None\n",
    "    z = model(data.x, data.train_pos_edge_index)\n",
    "    edge_index = torch.cat([data.val_pos_edge_index, data.val_neg_edge_index], dim=-1)\n",
    "    labels = get_link_labels(data.val_pos_edge_index, data.val_neg_edge_index).to(device)\n",
    "    pred = decode(z, edge_index).sigmoid()\n",
    "    auc = roc_auc_score(labels.cpu(), pred.cpu())\n",
    "    ap = average_precision_score(labels.cpu(), pred.cpu())\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a97fb1",
   "metadata": {},
   "source": [
    "## 主程序运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a934757a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running: Cora | GCN | Sampler=False | LR=0.005 | Layers=2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6930 | AUC: 0.6910 | AP: 0.7316\n",
      "Epoch 010 | Loss: 0.6696 | AUC: 0.6906 | AP: 0.7305\n",
      "Epoch 020 | Loss: 0.6376 | AUC: 0.7233 | AP: 0.7544\n",
      "Epoch 030 | Loss: 0.5657 | AUC: 0.7806 | AP: 0.7699\n",
      "Epoch 040 | Loss: 0.5192 | AUC: 0.8305 | AP: 0.8217\n",
      "Epoch 050 | Loss: 0.4861 | AUC: 0.8684 | AP: 0.8561\n",
      "Epoch 060 | Loss: 0.4779 | AUC: 0.8800 | AP: 0.8666\n",
      "Epoch 070 | Loss: 0.4672 | AUC: 0.8900 | AP: 0.8820\n",
      "Epoch 080 | Loss: 0.4619 | AUC: 0.8947 | AP: 0.8873\n",
      "Epoch 090 | Loss: 0.4599 | AUC: 0.8980 | AP: 0.8910\n",
      "Done. Total time: 3.54s\n",
      "\n",
      "--- Running: Cora | GCN | Sampler=False | LR=0.01 | Layers=3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6931 | AUC: 0.7285 | AP: 0.7439\n",
      "Epoch 010 | Loss: 0.6886 | AUC: 0.7076 | AP: 0.7320\n",
      "Epoch 020 | Loss: 0.6668 | AUC: 0.6879 | AP: 0.7318\n",
      "Epoch 030 | Loss: 0.5942 | AUC: 0.7977 | AP: 0.8095\n",
      "Epoch 040 | Loss: 0.5626 | AUC: 0.8056 | AP: 0.8221\n",
      "Epoch 050 | Loss: 0.5505 | AUC: 0.8145 | AP: 0.8356\n",
      "Epoch 060 | Loss: 0.5469 | AUC: 0.8164 | AP: 0.8404\n",
      "Epoch 070 | Loss: 0.5371 | AUC: 0.8266 | AP: 0.8462\n",
      "Epoch 080 | Loss: 0.5010 | AUC: 0.8623 | AP: 0.8620\n",
      "Epoch 090 | Loss: 0.4963 | AUC: 0.8605 | AP: 0.8635\n",
      "Done. Total time: 4.24s\n",
      "\n",
      "--- Running: Cora | GAT | Sampler=False | LR=0.005 | Layers=2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6929 | AUC: 0.6633 | AP: 0.6651\n",
      "Epoch 010 | Loss: 0.5741 | AUC: 0.7745 | AP: 0.7094\n",
      "Epoch 020 | Loss: 0.5279 | AUC: 0.8375 | AP: 0.8083\n",
      "Epoch 030 | Loss: 0.4823 | AUC: 0.8774 | AP: 0.8599\n",
      "Epoch 040 | Loss: 0.4721 | AUC: 0.8890 | AP: 0.8788\n",
      "Epoch 050 | Loss: 0.4629 | AUC: 0.8938 | AP: 0.8819\n",
      "Epoch 060 | Loss: 0.4489 | AUC: 0.8972 | AP: 0.8819\n",
      "Epoch 070 | Loss: 0.4469 | AUC: 0.9030 | AP: 0.8980\n",
      "Epoch 080 | Loss: 0.4431 | AUC: 0.8999 | AP: 0.9041\n",
      "Epoch 090 | Loss: 0.4415 | AUC: 0.8998 | AP: 0.9036\n",
      "Done. Total time: 9.44s\n",
      "\n",
      "--- Running: Cora | GAT | Sampler=False | LR=0.01 | Layers=3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6929 | AUC: 0.6144 | AP: 0.6607\n",
      "Epoch 010 | Loss: 0.6899 | AUC: 0.7096 | AP: 0.7099\n",
      "Epoch 020 | Loss: 0.5582 | AUC: 0.8108 | AP: 0.7729\n",
      "Epoch 030 | Loss: 0.5119 | AUC: 0.8483 | AP: 0.8027\n",
      "Epoch 040 | Loss: 0.4863 | AUC: 0.8541 | AP: 0.8077\n",
      "Epoch 050 | Loss: 0.4779 | AUC: 0.8632 | AP: 0.8227\n",
      "Epoch 060 | Loss: 0.4587 | AUC: 0.8749 | AP: 0.8563\n",
      "Epoch 070 | Loss: 0.4534 | AUC: 0.8889 | AP: 0.8802\n",
      "Epoch 080 | Loss: 0.4500 | AUC: 0.8892 | AP: 0.8889\n",
      "Epoch 090 | Loss: 0.4488 | AUC: 0.8896 | AP: 0.8859\n",
      "Done. Total time: 12.95s\n",
      "\n",
      "--- Running: Cora | GraphSAGE | Sampler=False | LR=0.005 | Layers=2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.7136 | AUC: 0.6076 | AP: 0.6195\n",
      "Epoch 010 | Loss: 0.6916 | AUC: 0.4979 | AP: 0.5343\n",
      "Epoch 020 | Loss: 0.6631 | AUC: 0.6440 | AP: 0.6194\n",
      "Epoch 030 | Loss: 0.5864 | AUC: 0.7294 | AP: 0.6841\n",
      "Epoch 040 | Loss: 0.5521 | AUC: 0.7831 | AP: 0.7533\n",
      "Epoch 050 | Loss: 0.5265 | AUC: 0.8062 | AP: 0.7778\n",
      "Epoch 060 | Loss: 0.5173 | AUC: 0.8219 | AP: 0.8060\n",
      "Epoch 070 | Loss: 0.4987 | AUC: 0.8290 | AP: 0.8234\n",
      "Epoch 080 | Loss: 0.4905 | AUC: 0.8414 | AP: 0.8409\n",
      "Epoch 090 | Loss: 0.4824 | AUC: 0.8533 | AP: 0.8515\n",
      "Done. Total time: 6.21s\n",
      "\n",
      "--- Running: Cora | GraphSAGE | Sampler=False | LR=0.01 | Layers=3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.7230 | AUC: 0.5694 | AP: 0.6072\n",
      "Epoch 010 | Loss: 0.6480 | AUC: 0.7549 | AP: 0.7241\n",
      "Epoch 020 | Loss: 0.5877 | AUC: 0.7447 | AP: 0.7281\n",
      "Epoch 030 | Loss: 0.5673 | AUC: 0.7704 | AP: 0.7512\n",
      "Epoch 040 | Loss: 0.5552 | AUC: 0.7794 | AP: 0.7593\n",
      "Epoch 050 | Loss: 0.5495 | AUC: 0.7802 | AP: 0.7653\n",
      "Epoch 060 | Loss: 0.5476 | AUC: 0.7805 | AP: 0.7620\n",
      "Epoch 070 | Loss: 0.5430 | AUC: 0.7833 | AP: 0.7689\n",
      "Epoch 080 | Loss: 0.5239 | AUC: 0.8046 | AP: 0.7992\n",
      "Epoch 090 | Loss: 0.5088 | AUC: 0.8138 | AP: 0.8191\n",
      "Done. Total time: 6.75s\n",
      "\n",
      "--- Running: Cora | GIN | Sampler=False | LR=0.005 | Layers=2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6910 | AUC: 0.5525 | AP: 0.6420\n",
      "Epoch 010 | Loss: 0.6459 | AUC: 0.6249 | AP: 0.6770\n",
      "Epoch 020 | Loss: 0.6235 | AUC: 0.5839 | AP: 0.6616\n",
      "Epoch 030 | Loss: 0.5986 | AUC: 0.5831 | AP: 0.6557\n",
      "Epoch 040 | Loss: 0.5615 | AUC: 0.6610 | AP: 0.7079\n",
      "Epoch 050 | Loss: 0.5136 | AUC: 0.7592 | AP: 0.7809\n",
      "Epoch 060 | Loss: 0.5020 | AUC: 0.8032 | AP: 0.8112\n",
      "Epoch 070 | Loss: 0.4767 | AUC: 0.8255 | AP: 0.8387\n",
      "Epoch 080 | Loss: 0.4599 | AUC: 0.8423 | AP: 0.8554\n",
      "Epoch 090 | Loss: 0.4607 | AUC: 0.8514 | AP: 0.8641\n",
      "Done. Total time: 5.69s\n",
      "\n",
      "--- Running: Cora | GIN | Sampler=False | LR=0.01 | Layers=3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.8082 | AUC: 0.5504 | AP: 0.5762\n",
      "Epoch 010 | Loss: 0.6625 | AUC: 0.6169 | AP: 0.6584\n",
      "Epoch 020 | Loss: 0.6139 | AUC: 0.6195 | AP: 0.6689\n",
      "Epoch 030 | Loss: 0.5787 | AUC: 0.6627 | AP: 0.6985\n",
      "Epoch 040 | Loss: 0.5750 | AUC: 0.6764 | AP: 0.7086\n",
      "Epoch 050 | Loss: 0.5379 | AUC: 0.6923 | AP: 0.7190\n",
      "Epoch 060 | Loss: 0.5347 | AUC: 0.6929 | AP: 0.7155\n",
      "Epoch 070 | Loss: 0.4941 | AUC: 0.7169 | AP: 0.7399\n",
      "Epoch 080 | Loss: 0.4830 | AUC: 0.7165 | AP: 0.7430\n",
      "Epoch 090 | Loss: 0.4888 | AUC: 0.7357 | AP: 0.7634\n",
      "Done. Total time: 5.99s\n",
      "\n",
      "--- Running: Citeseer | GCN | Sampler=False | LR=0.005 | Layers=2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6931 | AUC: 0.6902 | AP: 0.7023\n",
      "Epoch 010 | Loss: 0.6571 | AUC: 0.6409 | AP: 0.7050\n",
      "Epoch 020 | Loss: 0.5639 | AUC: 0.7882 | AP: 0.7884\n",
      "Epoch 030 | Loss: 0.5423 | AUC: 0.8004 | AP: 0.7940\n",
      "Epoch 040 | Loss: 0.5258 | AUC: 0.8131 | AP: 0.8046\n",
      "Epoch 050 | Loss: 0.5070 | AUC: 0.8371 | AP: 0.8343\n",
      "Epoch 060 | Loss: 0.4880 | AUC: 0.8675 | AP: 0.8653\n",
      "Epoch 070 | Loss: 0.4846 | AUC: 0.8606 | AP: 0.8617\n",
      "Epoch 080 | Loss: 0.4794 | AUC: 0.8581 | AP: 0.8574\n",
      "Epoch 090 | Loss: 0.4801 | AUC: 0.8549 | AP: 0.8524\n",
      "Done. Total time: 6.09s\n",
      "\n",
      "--- Running: Citeseer | GCN | Sampler=False | LR=0.01 | Layers=3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6931 | AUC: 0.6439 | AP: 0.6706\n",
      "Epoch 010 | Loss: 0.6452 | AUC: 0.7394 | AP: 0.7632\n",
      "Epoch 020 | Loss: 0.5413 | AUC: 0.7780 | AP: 0.7941\n",
      "Epoch 030 | Loss: 0.5016 | AUC: 0.8546 | AP: 0.8587\n",
      "Epoch 040 | Loss: 0.5029 | AUC: 0.8497 | AP: 0.8525\n",
      "Epoch 050 | Loss: 0.4773 | AUC: 0.8618 | AP: 0.8659\n",
      "Epoch 060 | Loss: 0.4835 | AUC: 0.8583 | AP: 0.8651\n",
      "Epoch 070 | Loss: 0.4678 | AUC: 0.8574 | AP: 0.8622\n",
      "Epoch 080 | Loss: 0.4668 | AUC: 0.8544 | AP: 0.8575\n",
      "Epoch 090 | Loss: 0.4600 | AUC: 0.8622 | AP: 0.8648\n",
      "Done. Total time: 6.37s\n",
      "\n",
      "--- Running: Citeseer | GAT | Sampler=False | LR=0.005 | Layers=2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6931 | AUC: 0.6685 | AP: 0.7156\n",
      "Epoch 010 | Loss: 0.5832 | AUC: 0.7987 | AP: 0.7989\n",
      "Epoch 020 | Loss: 0.5172 | AUC: 0.8602 | AP: 0.8578\n",
      "Epoch 030 | Loss: 0.4892 | AUC: 0.8748 | AP: 0.8775\n",
      "Epoch 040 | Loss: 0.4672 | AUC: 0.9060 | AP: 0.9113\n",
      "Epoch 050 | Loss: 0.4567 | AUC: 0.9110 | AP: 0.9125\n",
      "Epoch 060 | Loss: 0.4526 | AUC: 0.9048 | AP: 0.9099\n",
      "Epoch 070 | Loss: 0.4430 | AUC: 0.9083 | AP: 0.9154\n",
      "Epoch 080 | Loss: 0.4423 | AUC: 0.9033 | AP: 0.9135\n",
      "Epoch 090 | Loss: 0.4440 | AUC: 0.8975 | AP: 0.9083\n",
      "Done. Total time: 17.39s\n",
      "\n",
      "--- Running: Citeseer | GAT | Sampler=False | LR=0.01 | Layers=3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6931 | AUC: 0.5892 | AP: 0.6685\n",
      "Epoch 010 | Loss: 0.8700 | AUC: 0.6611 | AP: 0.7014\n",
      "Epoch 020 | Loss: 0.5886 | AUC: 0.8148 | AP: 0.8172\n",
      "Epoch 030 | Loss: 0.5088 | AUC: 0.8624 | AP: 0.8594\n",
      "Epoch 040 | Loss: 0.4697 | AUC: 0.8999 | AP: 0.8972\n",
      "Epoch 050 | Loss: 0.4512 | AUC: 0.9016 | AP: 0.8968\n",
      "Epoch 060 | Loss: 0.4499 | AUC: 0.8996 | AP: 0.8912\n",
      "Epoch 070 | Loss: 0.4421 | AUC: 0.8998 | AP: 0.9025\n",
      "Epoch 080 | Loss: 0.4483 | AUC: 0.8918 | AP: 0.8982\n",
      "Epoch 090 | Loss: 0.4411 | AUC: 0.8916 | AP: 0.8970\n",
      "Done. Total time: 21.35s\n",
      "\n",
      "--- Running: Citeseer | GraphSAGE | Sampler=False | LR=0.005 | Layers=2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.7092 | AUC: 0.6720 | AP: 0.6665\n",
      "Epoch 010 | Loss: 0.6772 | AUC: 0.6144 | AP: 0.6135\n",
      "Epoch 020 | Loss: 0.5848 | AUC: 0.7627 | AP: 0.7277\n",
      "Epoch 030 | Loss: 0.5587 | AUC: 0.7793 | AP: 0.7434\n",
      "Epoch 040 | Loss: 0.5400 | AUC: 0.7847 | AP: 0.7865\n",
      "Epoch 050 | Loss: 0.5420 | AUC: 0.7907 | AP: 0.8008\n",
      "Epoch 060 | Loss: 0.5313 | AUC: 0.7966 | AP: 0.8072\n",
      "Epoch 070 | Loss: 0.5235 | AUC: 0.8057 | AP: 0.8174\n",
      "Epoch 080 | Loss: 0.5200 | AUC: 0.8151 | AP: 0.8296\n",
      "Epoch 090 | Loss: 0.4984 | AUC: 0.8187 | AP: 0.8367\n",
      "Done. Total time: 13.49s\n",
      "\n",
      "--- Running: Citeseer | GraphSAGE | Sampler=False | LR=0.01 | Layers=3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.7184 | AUC: 0.5813 | AP: 0.6281\n",
      "Epoch 010 | Loss: 0.5990 | AUC: 0.6826 | AP: 0.6951\n",
      "Epoch 020 | Loss: 0.5644 | AUC: 0.7197 | AP: 0.7325\n",
      "Epoch 030 | Loss: 0.5510 | AUC: 0.7425 | AP: 0.7524\n",
      "Epoch 040 | Loss: 0.5363 | AUC: 0.7564 | AP: 0.7604\n",
      "Epoch 050 | Loss: 0.5267 | AUC: 0.7739 | AP: 0.7788\n",
      "Epoch 060 | Loss: 0.5085 | AUC: 0.7945 | AP: 0.7963\n",
      "Epoch 070 | Loss: 0.5036 | AUC: 0.7962 | AP: 0.7997\n",
      "Epoch 080 | Loss: 0.4981 | AUC: 0.8014 | AP: 0.8125\n",
      "Epoch 090 | Loss: 0.4888 | AUC: 0.8097 | AP: 0.8221\n",
      "Done. Total time: 14.04s\n",
      "\n",
      "--- Running: Citeseer | GIN | Sampler=False | LR=0.005 | Layers=2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.7129 | AUC: 0.5951 | AP: 0.6427\n",
      "Epoch 010 | Loss: 0.6303 | AUC: 0.6001 | AP: 0.6407\n",
      "Epoch 020 | Loss: 0.5813 | AUC: 0.6382 | AP: 0.6610\n",
      "Epoch 030 | Loss: 0.5541 | AUC: 0.6796 | AP: 0.6917\n",
      "Epoch 040 | Loss: 0.5062 | AUC: 0.7507 | AP: 0.7591\n",
      "Epoch 050 | Loss: 0.4908 | AUC: 0.7838 | AP: 0.7864\n",
      "Epoch 060 | Loss: 0.4830 | AUC: 0.8006 | AP: 0.8037\n",
      "Epoch 070 | Loss: 0.4663 | AUC: 0.8173 | AP: 0.8166\n",
      "Epoch 080 | Loss: 0.4652 | AUC: 0.8210 | AP: 0.8273\n",
      "Epoch 090 | Loss: 0.4574 | AUC: 0.8291 | AP: 0.8330\n",
      "Done. Total time: 11.86s\n",
      "\n",
      "--- Running: Citeseer | GIN | Sampler=False | LR=0.01 | Layers=3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss: 0.6590 | AUC: 0.6201 | AP: 0.6795\n",
      "Epoch 010 | Loss: 0.6248 | AUC: 0.6206 | AP: 0.6895\n",
      "Epoch 020 | Loss: 0.6300 | AUC: 0.6520 | AP: 0.7079\n",
      "Epoch 030 | Loss: 0.5659 | AUC: 0.7131 | AP: 0.7529\n",
      "Epoch 040 | Loss: 0.5223 | AUC: 0.7157 | AP: 0.7501\n",
      "Epoch 050 | Loss: 0.4945 | AUC: 0.7219 | AP: 0.7474\n",
      "Epoch 060 | Loss: 0.4681 | AUC: 0.7537 | AP: 0.7700\n",
      "Epoch 070 | Loss: 0.4623 | AUC: 0.7591 | AP: 0.7760\n",
      "Epoch 080 | Loss: 0.4699 | AUC: 0.7583 | AP: 0.7736\n",
      "Epoch 090 | Loss: 0.4601 | AUC: 0.7687 | AP: 0.7824\n",
      "Done. Total time: 12.26s\n",
      "\n",
      "--- Running: Flickr | GCN | Sampler=False | LR=0.005 | Layers=2 ---\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 000 | Loss: 3.6633 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 010 | Loss: 3.6620 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 020 | Loss: 1.9483 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 030 | Loss: 0.9863 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 040 | Loss: 0.6380 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 050 | Loss: 0.4400 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 060 | Loss: 0.6764 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 070 | Loss: 0.4330 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 080 | Loss: 0.4532 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 090 | Loss: 0.4182 (No link prediction)\n",
      "Done. Total time: 83.49s\n",
      "\n",
      "--- Running: Flickr | GCN | Sampler=False | LR=0.01 | Layers=3 ---\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 000 | Loss: 3.5369 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 010 | Loss: 5.6114 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 020 | Loss: 3.1294 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 030 | Loss: 1.8694 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 040 | Loss: 1.3353 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 050 | Loss: 0.9680 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 060 | Loss: 1.5190 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 070 | Loss: 1.4550 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 080 | Loss: 0.9420 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 090 | Loss: 1.0513 (No link prediction)\n",
      "Done. Total time: 115.83s\n",
      "\n",
      "--- Running: Flickr | GAT | Sampler=False | LR=0.005 | Layers=2 ---\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 000 | Loss: 7.6916 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 010 | Loss: 13.9265 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 020 | Loss: 7.2941 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 030 | Loss: 4.3241 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 040 | Loss: 4.2863 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 050 | Loss: 3.8680 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 060 | Loss: 3.5223 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 070 | Loss: 3.6448 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 080 | Loss: 3.5711 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 090 | Loss: 2.7478 (No link prediction)\n",
      "Done. Total time: 353.04s\n",
      "\n",
      "--- Running: Flickr | GAT | Sampler=False | LR=0.01 | Layers=3 ---\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 000 | Loss: 7.0038 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 010 | Loss: 44.3658 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 020 | Loss: 24.6899 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 030 | Loss: 19.0366 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 040 | Loss: 11.1440 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 050 | Loss: 8.9966 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 060 | Loss: 10.2671 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 070 | Loss: 6.5424 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 080 | Loss: 6.3437 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 090 | Loss: 6.9540 (No link prediction)\n",
      "Done. Total time: 616.52s\n",
      "\n",
      "--- Running: Flickr | GraphSAGE | Sampler=False | LR=0.005 | Layers=2 ---\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 000 | Loss: 189.9280 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 010 | Loss: 30.3894 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 020 | Loss: 11.9878 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 030 | Loss: 6.7439 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 040 | Loss: 4.3766 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 050 | Loss: 5.8487 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 060 | Loss: 2.5519 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 070 | Loss: 2.5202 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 080 | Loss: 3.7340 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 090 | Loss: 6.0897 (No link prediction)\n",
      "Done. Total time: 143.94s\n",
      "\n",
      "--- Running: Flickr | GraphSAGE | Sampler=False | LR=0.01 | Layers=3 ---\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 000 | Loss: 193.8229 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 010 | Loss: 17.9070 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 020 | Loss: 6.8770 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 030 | Loss: 6.6701 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 040 | Loss: 2.7420 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 050 | Loss: 2.1146 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 060 | Loss: 3.9424 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 070 | Loss: 3.4634 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 080 | Loss: 2.9414 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 090 | Loss: 3.1877 (No link prediction)\n",
      "Done. Total time: 169.16s\n",
      "\n",
      "--- Running: Flickr | GIN | Sampler=False | LR=0.005 | Layers=2 ---\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 000 | Loss: 1371.6429 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 010 | Loss: 107.0273 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 020 | Loss: 28.1082 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 030 | Loss: 15.5832 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 040 | Loss: 6.9761 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 050 | Loss: 3.3762 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 060 | Loss: 1.7181 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 070 | Loss: 0.7832 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 080 | Loss: 0.5800 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 090 | Loss: 0.5627 (No link prediction)\n",
      "Done. Total time: 135.52s\n",
      "\n",
      "--- Running: Flickr | GIN | Sampler=False | LR=0.01 | Layers=3 ---\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 000 | Loss: 8486.6455 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 010 | Loss: 126.0062 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 020 | Loss: 25.6985 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 030 | Loss: 19.7553 (No link prediction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No link prediction evaluation for this dataset.\n",
      "Epoch 040 | Loss: 10.8807 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 050 | Loss: 3.4849 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 060 | Loss: 2.6293 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 070 | Loss: 1.7085 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 080 | Loss: 1.2998 (No link prediction)\n",
      "No link prediction evaluation for this dataset.\n",
      "Epoch 090 | Loss: 0.8838 (No link prediction)\n",
      "Done. Total time: 161.58s\n"
     ]
    }
   ],
   "source": [
    "def run(dataset_name, model_name, use_sampler=False, lr=0.01, layers=2, hidden_dim=64, epochs=100):\n",
    "    print(f\"\\n--- Running: {dataset_name} | {model_name} | Sampler={use_sampler} | LR={lr} | Layers={layers} ---\")\n",
    "    data, in_dim = load_data(dataset_name)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 模型选择\n",
    "    if model_name == 'GCN': model = GCNEncoder(in_dim, hidden_dim, layers)\n",
    "    elif model_name == 'GAT': model = GATEncoder(in_dim, hidden_dim, layers)\n",
    "    elif model_name == 'GraphSAGE': model = SAGEEncoder(in_dim, hidden_dim, layers)\n",
    "    elif model_name == 'GIN': model = GINEncoder(in_dim, hidden_dim, layers)\n",
    "    else: raise ValueError(\"Unsupported model\")\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model, data, optimizer, device)\n",
    "        if epoch % 10 == 0:\n",
    "            auc, ap = test(model, data, device)\n",
    "            if auc is not None:\n",
    "                print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | AUC: {auc:.4f} | AP: {ap:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} (No link prediction)\")\n",
    "    print(f\"Done. Total time: {time.time() - start:.2f}s\")\n",
    "    \n",
    "# ======== 启动入口 ========\n",
    "if __name__ == \"__main__\":\n",
    "    datasets = ['Cora', 'Citeseer', 'Flickr']\n",
    "    models = ['GCN', 'GAT', 'GraphSAGE', 'GIN']\n",
    "    for d in datasets:\n",
    "        for m in models:\n",
    "            run(d, m, use_sampler=False, lr=0.005, layers=2)\n",
    "            run(d, m, use_sampler=False, lr=0.01, layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01f56c",
   "metadata": {},
   "source": [
    "# 图分类\n",
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2b05361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择数据集，支持 TUDataset 或 ZINC 数据集\n",
    "def load_data(name):\n",
    "    if name == 'TUDataset':\n",
    "        dataset = TUDataset(root='D:\\Data\\master\\Graph Machine Learning\\GNN\\standard benchmark datasets\\TUDataset', name='PROTEINS')\n",
    "    elif name == 'ZINC':\n",
    "        dataset = ZINC(root='D:\\Data\\master\\Graph Machine Learning\\GNN\\standard benchmark datasets\\ZINC')\n",
    "    else:\n",
    "        raise ValueError(\"Only 'TUDataset' and 'ZINC' are supported.\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494a1c2",
   "metadata": {},
   "source": [
    "## 定义模型和方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c15a4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''池化方法'''\n",
    "class GraphPooling(nn.Module):\n",
    "    def __init__(self, pooling_type='mean'):\n",
    "        super(GraphPooling, self).__init__()\n",
    "        if pooling_type == 'mean':\n",
    "            self.pool = global_mean_pool\n",
    "        elif pooling_type == 'max':\n",
    "            self.pool = global_max_pool\n",
    "        elif pooling_type == 'min':\n",
    "            self.pool = global_add_pool  # Replace with MinPooling logic if needed\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pooling type. Choose from ['mean', 'max', 'min'].\")\n",
    "    def forward(self, x, batch):\n",
    "        return self.pool(x, batch)\n",
    "    \n",
    "'''GCN模型'''\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, pooling, layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "        self.pool = pooling\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()  \n",
    "        edge_weight = x.new_ones(edge_index.size(1))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index, edge_weight=edge_weight))\n",
    "        x = self.pool(x, batch)\n",
    "        return self.lin(x)\n",
    "    \n",
    "'''GAT模型'''\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, pooling, layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels))\n",
    "        for _ in range(layers - 2):\n",
    "            self.convs.append(GATConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GATConv(hidden_channels, hidden_channels))\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "        self.pool = pooling\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = self.pool(x, batch)\n",
    "        return self.lin(x)\n",
    "    \n",
    "'''GraphSAGE模型'''\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, pooling, layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "        self.pool = pooling\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = self.pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "'''GIN模型'''\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, pooling, layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(layers):\n",
    "            mlp = Sequential(Linear(in_channels if i == 0 else hidden_channels, hidden_channels),\n",
    "                             ReLU(),\n",
    "                             Linear(hidden_channels, hidden_channels))\n",
    "            self.convs.append(GINConv(mlp))\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "        self.pool = pooling\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = self.pool(x, batch)\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edf7eb",
   "metadata": {},
   "source": [
    "## 训练与检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eaba67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义训练过程'''\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1) if out.shape[1] > 1 else out.view(-1)\n",
    "        if out.shape[1] > 1:\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "    acc = correct / len(loader.dataset) if out.shape[1] > 1 else 0.0\n",
    "    return total_loss / len(loader), acc\n",
    "\n",
    "'''定义测试过程'''\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1) if out.shape[1] > 1 else out.view(-1)\n",
    "        if out.shape[1] > 1:\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "    acc = correct / len(loader.dataset) if out.shape[1] > 1 else 0.0\n",
    "    return acc\n",
    "\n",
    "'''设置训练和测试过程'''\n",
    "def run_all():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    datasets = ['TUDataset', 'ZINC']\n",
    "    models = ['GCN', 'GAT', 'GraphSAGE', 'GIN']\n",
    "    poolings = ['mean', 'max', 'min']\n",
    "    lrs = [0.001, 0.005]\n",
    "    layers_list = [2, 3]\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        dataset = load_data(dataset_name)\n",
    "        dataset = dataset.shuffle()\n",
    "        in_dim = dataset.num_node_features\n",
    "        out_dim = dataset.num_classes if dataset_name == 'TUDataset' else 1\n",
    "\n",
    "        for pooling_type in poolings:\n",
    "            pool = GraphPooling(pooling_type)\n",
    "            for model_name in models:\n",
    "                for lr in lrs:\n",
    "                    for layers in layers_list:\n",
    "                        train_loader = DataLoader(dataset[:int(0.8*len(dataset))], batch_size=32, shuffle=True)\n",
    "                        test_loader = DataLoader(dataset[int(0.8*len(dataset)):], batch_size=32)\n",
    "\n",
    "                        if model_name == 'GCN':\n",
    "                            model = GCN(in_dim, 64, out_dim, pool, layers).to(device)\n",
    "                        elif model_name == 'GAT':\n",
    "                            model = GAT(in_dim, 64, out_dim, pool, layers).to(device)\n",
    "                        elif model_name == 'GraphSAGE':\n",
    "                            model = GraphSAGE(in_dim, 64, out_dim, pool, layers).to(device)\n",
    "                        elif model_name == 'GIN':\n",
    "                            model = GIN(in_dim, 64, out_dim, pool, layers).to(device)\n",
    "\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                        criterion = nn.CrossEntropyLoss() if dataset_name == 'TUDataset' else nn.L1Loss()\n",
    "\n",
    "                        start = time.time()\n",
    "                        for epoch in range(1, 51):\n",
    "                            train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "                            test_acc = test(model, test_loader, device)\n",
    "                            if epoch % 10 == 0:\n",
    "                                print(f\"[{dataset_name}] {model_name} | Pool={pooling_type} | LR={lr} | Layers={layers} | Epoch={epoch} | Loss={train_loss:.4f} | TrainAcc={train_acc:.4f} | TestAcc={test_acc:.4f}\")\n",
    "                        print(f\"Total Time: {time.time() - start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38acae4c",
   "metadata": {},
   "source": [
    "## 主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89eb5654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.6388 | TrainAcc=0.6528 | TestAcc=0.7534\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.6229 | TrainAcc=0.7000 | TestAcc=0.7444\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.6135 | TrainAcc=0.7034 | TestAcc=0.7444\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.6086 | TrainAcc=0.7101 | TestAcc=0.7354\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.6090 | TrainAcc=0.7067 | TestAcc=0.7534\n",
      "Total Time: 13.86s\n",
      "\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.6272 | TrainAcc=0.6775 | TestAcc=0.7309\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.6144 | TrainAcc=0.7011 | TestAcc=0.7220\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.6066 | TrainAcc=0.7000 | TestAcc=0.7578\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.6065 | TrainAcc=0.7011 | TestAcc=0.7489\n",
      "[TUDataset] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.6058 | TrainAcc=0.7079 | TestAcc=0.7444\n",
      "Total Time: 17.53s\n",
      "\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.6219 | TrainAcc=0.6944 | TestAcc=0.6457\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.6123 | TrainAcc=0.7067 | TestAcc=0.7489\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.6144 | TrainAcc=0.6955 | TestAcc=0.7309\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.6044 | TrainAcc=0.7112 | TestAcc=0.7399\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.6032 | TrainAcc=0.6899 | TestAcc=0.6951\n",
      "Total Time: 14.11s\n",
      "\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.6222 | TrainAcc=0.6843 | TestAcc=0.6951\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.6088 | TrainAcc=0.7011 | TestAcc=0.7309\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.6070 | TrainAcc=0.6978 | TestAcc=0.7399\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.6049 | TrainAcc=0.7112 | TestAcc=0.7534\n",
      "[TUDataset] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.6031 | TrainAcc=0.7011 | TestAcc=0.7354\n",
      "Total Time: 17.45s\n",
      "\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.6383 | TrainAcc=0.6539 | TestAcc=0.6771\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.6078 | TrainAcc=0.6989 | TestAcc=0.7309\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.6028 | TrainAcc=0.7067 | TestAcc=0.7265\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.6026 | TrainAcc=0.7056 | TestAcc=0.7265\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.5992 | TrainAcc=0.7067 | TestAcc=0.7354\n",
      "Total Time: 19.53s\n",
      "\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.6108 | TrainAcc=0.6966 | TestAcc=0.7354\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5975 | TrainAcc=0.7011 | TestAcc=0.7399\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.5978 | TrainAcc=0.7011 | TestAcc=0.7489\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5913 | TrainAcc=0.7000 | TestAcc=0.7399\n",
      "[TUDataset] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5928 | TrainAcc=0.7124 | TestAcc=0.7444\n",
      "Total Time: 27.01s\n",
      "\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.6133 | TrainAcc=0.6831 | TestAcc=0.7444\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.6074 | TrainAcc=0.6955 | TestAcc=0.7175\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.6007 | TrainAcc=0.7157 | TestAcc=0.7399\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5992 | TrainAcc=0.7000 | TestAcc=0.7309\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5965 | TrainAcc=0.7101 | TestAcc=0.7220\n",
      "Total Time: 19.72s\n",
      "\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.6099 | TrainAcc=0.6865 | TestAcc=0.7265\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5994 | TrainAcc=0.7180 | TestAcc=0.7489\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5972 | TrainAcc=0.7202 | TestAcc=0.7444\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.5946 | TrainAcc=0.7011 | TestAcc=0.7444\n",
      "[TUDataset] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.5915 | TrainAcc=0.7011 | TestAcc=0.7309\n",
      "Total Time: 27.75s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.6287 | TrainAcc=0.6719 | TestAcc=0.7309\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.6155 | TrainAcc=0.6910 | TestAcc=0.7309\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.6149 | TrainAcc=0.6978 | TestAcc=0.7399\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.6060 | TrainAcc=0.7067 | TestAcc=0.7354\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.6089 | TrainAcc=0.6944 | TestAcc=0.7265\n",
      "Total Time: 11.71s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.6291 | TrainAcc=0.6876 | TestAcc=0.7444\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.6121 | TrainAcc=0.7045 | TestAcc=0.7309\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.6089 | TrainAcc=0.6966 | TestAcc=0.7534\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.6087 | TrainAcc=0.6899 | TestAcc=0.7399\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5981 | TrainAcc=0.7079 | TestAcc=0.7085\n",
      "Total Time: 15.62s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.6212 | TrainAcc=0.6708 | TestAcc=0.6816\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.6239 | TrainAcc=0.6775 | TestAcc=0.6682\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.6126 | TrainAcc=0.6876 | TestAcc=0.7130\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.6090 | TrainAcc=0.6820 | TestAcc=0.6951\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5963 | TrainAcc=0.7034 | TestAcc=0.7040\n",
      "Total Time: 12.13s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.6270 | TrainAcc=0.6730 | TestAcc=0.6996\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.6046 | TrainAcc=0.6933 | TestAcc=0.7130\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5966 | TrainAcc=0.6978 | TestAcc=0.7175\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.6011 | TrainAcc=0.6944 | TestAcc=0.7220\n",
      "[TUDataset] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.5806 | TrainAcc=0.7079 | TestAcc=0.7220\n",
      "Total Time: 15.65s\n",
      "\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.6180 | TrainAcc=0.7000 | TestAcc=0.6951\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.6080 | TrainAcc=0.7034 | TestAcc=0.7309\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5990 | TrainAcc=0.7112 | TestAcc=0.7489\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5971 | TrainAcc=0.7056 | TestAcc=0.7354\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.6033 | TrainAcc=0.6876 | TestAcc=0.7399\n",
      "Total Time: 13.84s\n",
      "\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.6192 | TrainAcc=0.6764 | TestAcc=0.7444\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.6234 | TrainAcc=0.6742 | TestAcc=0.7085\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.6091 | TrainAcc=0.6955 | TestAcc=0.7534\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5955 | TrainAcc=0.7135 | TestAcc=0.7444\n",
      "[TUDataset] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5835 | TrainAcc=0.7225 | TestAcc=0.7399\n",
      "Total Time: 17.71s\n",
      "\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.6212 | TrainAcc=0.6966 | TestAcc=0.7040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.6061 | TrainAcc=0.7056 | TestAcc=0.7354\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.6035 | TrainAcc=0.7101 | TestAcc=0.7399\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.6090 | TrainAcc=0.6944 | TestAcc=0.7399\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5914 | TrainAcc=0.7236 | TestAcc=0.7444\n",
      "Total Time: 13.52s\n",
      "\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.6267 | TrainAcc=0.6697 | TestAcc=0.7399\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.6048 | TrainAcc=0.7022 | TestAcc=0.7399\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.6000 | TrainAcc=0.6989 | TestAcc=0.7399\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.5948 | TrainAcc=0.7101 | TestAcc=0.7175\n",
      "[TUDataset] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.6014 | TrainAcc=0.7090 | TestAcc=0.7399\n",
      "Total Time: 17.85s\n",
      "\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.5893 | TrainAcc=0.7067 | TestAcc=0.7623\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.5519 | TrainAcc=0.7348 | TestAcc=0.7623\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5470 | TrainAcc=0.7416 | TestAcc=0.7668\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5462 | TrainAcc=0.7404 | TestAcc=0.7668\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.5479 | TrainAcc=0.7404 | TestAcc=0.7713\n",
      "Total Time: 15.67s\n",
      "\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.5803 | TrainAcc=0.6966 | TestAcc=0.6637\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5318 | TrainAcc=0.7483 | TestAcc=0.7578\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.5283 | TrainAcc=0.7517 | TestAcc=0.7758\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5211 | TrainAcc=0.7562 | TestAcc=0.7444\n",
      "[TUDataset] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5166 | TrainAcc=0.7629 | TestAcc=0.7489\n",
      "Total Time: 20.07s\n",
      "\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.5555 | TrainAcc=0.7315 | TestAcc=0.7489\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.5506 | TrainAcc=0.7315 | TestAcc=0.7623\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.5332 | TrainAcc=0.7371 | TestAcc=0.7578\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5251 | TrainAcc=0.7494 | TestAcc=0.7848\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5245 | TrainAcc=0.7494 | TestAcc=0.7937\n",
      "Total Time: 15.43s\n",
      "\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.5560 | TrainAcc=0.7292 | TestAcc=0.7220\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5340 | TrainAcc=0.7371 | TestAcc=0.7309\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5264 | TrainAcc=0.7416 | TestAcc=0.7937\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.5159 | TrainAcc=0.7652 | TestAcc=0.7623\n",
      "[TUDataset] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.5048 | TrainAcc=0.7562 | TestAcc=0.7758\n",
      "Total Time: 20.49s\n",
      "\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.5834 | TrainAcc=0.7315 | TestAcc=0.7578\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.5561 | TrainAcc=0.7281 | TestAcc=0.7668\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5455 | TrainAcc=0.7326 | TestAcc=0.7668\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5401 | TrainAcc=0.7382 | TestAcc=0.7578\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.5371 | TrainAcc=0.7348 | TestAcc=0.7578\n",
      "Total Time: 21.46s\n",
      "\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.5535 | TrainAcc=0.7348 | TestAcc=0.7534\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5400 | TrainAcc=0.7371 | TestAcc=0.7534\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.5293 | TrainAcc=0.7551 | TestAcc=0.7668\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5314 | TrainAcc=0.7562 | TestAcc=0.7892\n",
      "[TUDataset] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5345 | TrainAcc=0.7506 | TestAcc=0.7713\n",
      "Total Time: 27.96s\n",
      "\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.5722 | TrainAcc=0.7157 | TestAcc=0.7265\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.5461 | TrainAcc=0.7348 | TestAcc=0.7534\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.5379 | TrainAcc=0.7404 | TestAcc=0.7399\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5338 | TrainAcc=0.7449 | TestAcc=0.7623\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5404 | TrainAcc=0.7472 | TestAcc=0.7668\n",
      "Total Time: 21.26s\n",
      "\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.5554 | TrainAcc=0.7393 | TestAcc=0.7130\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5630 | TrainAcc=0.7213 | TestAcc=0.7713\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5211 | TrainAcc=0.7528 | TestAcc=0.7444\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.5288 | TrainAcc=0.7348 | TestAcc=0.7578\n",
      "[TUDataset] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.5214 | TrainAcc=0.7539 | TestAcc=0.7578\n",
      "Total Time: 27.81s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.5412 | TrainAcc=0.7483 | TestAcc=0.7578\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.5296 | TrainAcc=0.7461 | TestAcc=0.7489\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5290 | TrainAcc=0.7517 | TestAcc=0.7713\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5226 | TrainAcc=0.7584 | TestAcc=0.7534\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.5191 | TrainAcc=0.7607 | TestAcc=0.7803\n",
      "Total Time: 12.95s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.5328 | TrainAcc=0.7416 | TestAcc=0.7623\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5281 | TrainAcc=0.7517 | TestAcc=0.7713\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.5257 | TrainAcc=0.7551 | TestAcc=0.7534\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5119 | TrainAcc=0.7708 | TestAcc=0.7578\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5061 | TrainAcc=0.7573 | TestAcc=0.7803\n",
      "Total Time: 16.79s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.5561 | TrainAcc=0.7258 | TestAcc=0.7713\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.5273 | TrainAcc=0.7483 | TestAcc=0.7713\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.5069 | TrainAcc=0.7584 | TestAcc=0.7758\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5033 | TrainAcc=0.7674 | TestAcc=0.7848\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5000 | TrainAcc=0.7685 | TestAcc=0.7758\n",
      "Total Time: 12.89s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.5361 | TrainAcc=0.7494 | TestAcc=0.7713\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5152 | TrainAcc=0.7562 | TestAcc=0.7803\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5328 | TrainAcc=0.7494 | TestAcc=0.7489\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.4848 | TrainAcc=0.7685 | TestAcc=0.7713\n",
      "[TUDataset] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.4934 | TrainAcc=0.7753 | TestAcc=0.7578\n",
      "Total Time: 16.60s\n",
      "\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.5475 | TrainAcc=0.7506 | TestAcc=0.7399\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.5245 | TrainAcc=0.7607 | TestAcc=0.7803\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5126 | TrainAcc=0.7618 | TestAcc=0.7803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5027 | TrainAcc=0.7674 | TestAcc=0.7713\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.4965 | TrainAcc=0.7708 | TestAcc=0.7848\n",
      "Total Time: 14.82s\n",
      "\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.5413 | TrainAcc=0.7472 | TestAcc=0.7803\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5161 | TrainAcc=0.7697 | TestAcc=0.7758\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.4967 | TrainAcc=0.7640 | TestAcc=0.7848\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.4813 | TrainAcc=0.7854 | TestAcc=0.7982\n",
      "[TUDataset] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.4665 | TrainAcc=0.7820 | TestAcc=0.7758\n",
      "Total Time: 19.18s\n",
      "\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.5706 | TrainAcc=0.7101 | TestAcc=0.7309\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.5354 | TrainAcc=0.7472 | TestAcc=0.7668\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.5218 | TrainAcc=0.7674 | TestAcc=0.7668\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5220 | TrainAcc=0.7674 | TestAcc=0.7848\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5150 | TrainAcc=0.7528 | TestAcc=0.7758\n",
      "Total Time: 14.49s\n",
      "\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.5507 | TrainAcc=0.7303 | TestAcc=0.7713\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5585 | TrainAcc=0.7101 | TestAcc=0.7578\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5409 | TrainAcc=0.7337 | TestAcc=0.7713\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.4976 | TrainAcc=0.7685 | TestAcc=0.8027\n",
      "[TUDataset] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.4890 | TrainAcc=0.7652 | TestAcc=0.7937\n",
      "Total Time: 19.15s\n",
      "\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.5546 | TrainAcc=0.7169 | TestAcc=0.7399\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.5384 | TrainAcc=0.7416 | TestAcc=0.7265\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5355 | TrainAcc=0.7449 | TestAcc=0.7578\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5398 | TrainAcc=0.7494 | TestAcc=0.7713\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.5390 | TrainAcc=0.7494 | TestAcc=0.7534\n",
      "Total Time: 14.18s\n",
      "\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.5552 | TrainAcc=0.7067 | TestAcc=0.7309\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5477 | TrainAcc=0.7213 | TestAcc=0.7444\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.5368 | TrainAcc=0.7483 | TestAcc=0.7444\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5322 | TrainAcc=0.7562 | TestAcc=0.7130\n",
      "[TUDataset] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5281 | TrainAcc=0.7449 | TestAcc=0.7354\n",
      "Total Time: 18.52s\n",
      "\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.5487 | TrainAcc=0.7551 | TestAcc=0.7489\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.5344 | TrainAcc=0.7427 | TestAcc=0.7713\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.5417 | TrainAcc=0.7315 | TestAcc=0.7130\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5451 | TrainAcc=0.7292 | TestAcc=0.6547\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5349 | TrainAcc=0.7427 | TestAcc=0.7534\n",
      "Total Time: 14.49s\n",
      "\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.5446 | TrainAcc=0.7438 | TestAcc=0.7265\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5462 | TrainAcc=0.7371 | TestAcc=0.7489\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5372 | TrainAcc=0.7427 | TestAcc=0.7309\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.5410 | TrainAcc=0.7348 | TestAcc=0.7220\n",
      "[TUDataset] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.5263 | TrainAcc=0.7348 | TestAcc=0.7175\n",
      "Total Time: 19.06s\n",
      "\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.5639 | TrainAcc=0.7202 | TestAcc=0.7623\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.5395 | TrainAcc=0.7416 | TestAcc=0.7265\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5473 | TrainAcc=0.7337 | TestAcc=0.7623\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5410 | TrainAcc=0.7461 | TestAcc=0.7623\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.5286 | TrainAcc=0.7449 | TestAcc=0.7713\n",
      "Total Time: 20.63s\n",
      "\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.5551 | TrainAcc=0.7281 | TestAcc=0.7130\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5427 | TrainAcc=0.7494 | TestAcc=0.7534\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.5377 | TrainAcc=0.7551 | TestAcc=0.7354\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5328 | TrainAcc=0.7472 | TestAcc=0.7354\n",
      "[TUDataset] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5279 | TrainAcc=0.7483 | TestAcc=0.7668\n",
      "Total Time: 27.42s\n",
      "\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.5430 | TrainAcc=0.7449 | TestAcc=0.7578\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.5375 | TrainAcc=0.7404 | TestAcc=0.7265\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.5344 | TrainAcc=0.7326 | TestAcc=0.7444\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5423 | TrainAcc=0.7449 | TestAcc=0.7489\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5278 | TrainAcc=0.7438 | TestAcc=0.7489\n",
      "Total Time: 19.20s\n",
      "\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.5572 | TrainAcc=0.7191 | TestAcc=0.7578\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5418 | TrainAcc=0.7326 | TestAcc=0.7578\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5328 | TrainAcc=0.7528 | TestAcc=0.7354\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.5331 | TrainAcc=0.7483 | TestAcc=0.7444\n",
      "[TUDataset] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.5281 | TrainAcc=0.7393 | TestAcc=0.7578\n",
      "Total Time: 26.26s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.5831 | TrainAcc=0.6876 | TestAcc=0.7668\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.5535 | TrainAcc=0.7348 | TestAcc=0.7085\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5625 | TrainAcc=0.7315 | TestAcc=0.7040\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5421 | TrainAcc=0.7427 | TestAcc=0.7399\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.5366 | TrainAcc=0.7449 | TestAcc=0.7578\n",
      "Total Time: 13.02s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.5686 | TrainAcc=0.6843 | TestAcc=0.6861\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5511 | TrainAcc=0.7303 | TestAcc=0.7444\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.5459 | TrainAcc=0.7315 | TestAcc=0.7444\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5312 | TrainAcc=0.7461 | TestAcc=0.7623\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5435 | TrainAcc=0.7438 | TestAcc=0.7489\n",
      "Total Time: 15.40s\n",
      "\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.5816 | TrainAcc=0.7191 | TestAcc=0.7713\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.5369 | TrainAcc=0.7382 | TestAcc=0.7489\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.5283 | TrainAcc=0.7472 | TestAcc=0.7309\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5308 | TrainAcc=0.7382 | TestAcc=0.7399\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5265 | TrainAcc=0.7371 | TestAcc=0.7578\n",
      "Total Time: 11.70s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.5491 | TrainAcc=0.7483 | TestAcc=0.7085\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5402 | TrainAcc=0.7326 | TestAcc=0.7489\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5382 | TrainAcc=0.7348 | TestAcc=0.7175\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.5204 | TrainAcc=0.7461 | TestAcc=0.7309\n",
      "[TUDataset] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.5268 | TrainAcc=0.7438 | TestAcc=0.7444\n",
      "Total Time: 15.68s\n",
      "\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=10 | Loss=0.5649 | TrainAcc=0.6888 | TestAcc=0.7444\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=20 | Loss=0.5408 | TrainAcc=0.7337 | TestAcc=0.7354\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=30 | Loss=0.5406 | TrainAcc=0.7371 | TestAcc=0.7534\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=40 | Loss=0.5264 | TrainAcc=0.7618 | TestAcc=0.7578\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=50 | Loss=0.5202 | TrainAcc=0.7573 | TestAcc=0.7354\n",
      "Total Time: 13.51s\n",
      "\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=3 | Epoch=10 | Loss=0.5805 | TrainAcc=0.7169 | TestAcc=0.7399\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=3 | Epoch=20 | Loss=0.5344 | TrainAcc=0.7371 | TestAcc=0.7220\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=3 | Epoch=30 | Loss=0.5372 | TrainAcc=0.7584 | TestAcc=0.7623\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=3 | Epoch=40 | Loss=0.5320 | TrainAcc=0.7315 | TestAcc=0.7489\n",
      "[TUDataset] GIN | Pool=min | LR=0.001 | Layers=3 | Epoch=50 | Loss=0.5208 | TrainAcc=0.7596 | TestAcc=0.7713\n",
      "Total Time: 17.63s\n",
      "\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=2 | Epoch=10 | Loss=0.5467 | TrainAcc=0.7382 | TestAcc=0.7399\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=2 | Epoch=20 | Loss=0.5402 | TrainAcc=0.7393 | TestAcc=0.7444\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=2 | Epoch=30 | Loss=0.5381 | TrainAcc=0.7416 | TestAcc=0.7713\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=2 | Epoch=40 | Loss=0.5448 | TrainAcc=0.7360 | TestAcc=0.7578\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=2 | Epoch=50 | Loss=0.5184 | TrainAcc=0.7449 | TestAcc=0.7534\n",
      "Total Time: 13.35s\n",
      "\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=3 | Epoch=10 | Loss=0.5561 | TrainAcc=0.7393 | TestAcc=0.7623\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=3 | Epoch=20 | Loss=0.5789 | TrainAcc=0.7281 | TestAcc=0.7175\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=3 | Epoch=30 | Loss=0.5389 | TrainAcc=0.7404 | TestAcc=0.7040\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=3 | Epoch=40 | Loss=0.5418 | TrainAcc=0.7427 | TestAcc=0.6996\n",
      "[TUDataset] GIN | Pool=min | LR=0.005 | Layers=3 | Epoch=50 | Loss=0.5261 | TrainAcc=0.7506 | TestAcc=0.7354\n",
      "Total Time: 18.45s\n",
      "\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2435.92s\n",
      "\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2993.61s\n",
      "\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2462.16s\n",
      "\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=mean | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3055.44s\n",
      "\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3173.37s\n",
      "\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 4129.42s\n",
      "\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4956 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3196.23s\n",
      "\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=mean | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 4265.50s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4948 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2232.47s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4948 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4947 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ZINC] GraphSAGE | Pool=mean | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4948 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2847.07s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2207.68s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=mean | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2836.99s\n",
      "\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2506.03s\n",
      "\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3241.98s\n",
      "\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2509.70s\n",
      "\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4956 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=mean | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3324.07s\n",
      "\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2899.82s\n",
      "\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3028.38s\n",
      "\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2331.09s\n",
      "\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=max | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2857.10s\n",
      "\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2957.42s\n",
      "\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3814.54s\n",
      "\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2944.10s\n",
      "\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4956 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=max | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4957 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3830.94s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2108.99s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4949 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2603.77s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4958 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2102.86s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=max | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2641.21s\n",
      "\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2400.27s\n",
      "\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3055.51s\n",
      "\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2400.42s\n",
      "\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=max | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3112.34s\n",
      "\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2498.90s\n",
      "\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3019.25s\n",
      "\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2339.34s\n",
      "\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GCN | Pool=min | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2693.73s\n",
      "\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2820.71s\n",
      "\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3827.06s\n",
      "\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4956 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 3249.42s\n",
      "\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ZINC] GAT | Pool=min | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 4070.02s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=2 | Epoch=50 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 1985.13s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=10 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=20 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=30 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=40 | Loss=1.4950 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.001 | Layers=3 | Epoch=50 | Loss=1.4951 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2528.54s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=10 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=30 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=40 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=2 | Epoch=50 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 1968.43s\n",
      "\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=10 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=20 | Loss=1.4953 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=30 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=40 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GraphSAGE | Pool=min | LR=0.005 | Layers=3 | Epoch=50 | Loss=1.4955 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "Total Time: 2532.52s\n",
      "\n",
      "[ZINC] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=10 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=20 | Loss=1.4954 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=30 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n",
      "[ZINC] GIN | Pool=min | LR=0.001 | Layers=2 | Epoch=40 | Loss=1.4952 | TrainAcc=0.0000 | TestAcc=0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37412\\559479271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mrun_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37412\\3674035763.py\u001b[0m in \u001b[0;36mrun_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m                         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m51\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                             \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37412\\3674035763.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37412\\2056463934.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch_geometric\\nn\\conv\\gin_conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# propagate_type: (x: OptPairTensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mx_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gin_conv_GINConv_propagate_c32ior9p.py\u001b[0m in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, x, size)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# End Aggregate Forward Pre Hook #######################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         out = self.aggregate(\n\u001b[0m\u001b[0;32m    230\u001b[0m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mas\u001b[0m \u001b[0mspecified\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maggr\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0margument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \"\"\"\n\u001b[1;32m--> 594\u001b[1;33m         return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,\n\u001b[0m\u001b[0;32m    595\u001b[0m                                 dim=self.node_dim)\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch_geometric\\experimental.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_experimental_mode_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'disable_dynamic_shapes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrequired_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrequired_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             return super().__call__(x, index=index, ptr=ptr, dim_size=dim_size,\n\u001b[0m\u001b[0;32m    132\u001b[0m                                     dim=dim, **kwargs)\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch_geometric\\nn\\aggr\\basic.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mptr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 dim: int = -2) -> Tensor:\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Aggregation requires 'index' to be specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     def to_dense_batch(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch_geometric\\utils\\_scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sum'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'add'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
