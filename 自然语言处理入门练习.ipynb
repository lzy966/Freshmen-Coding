{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ecde017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import tqdm\n",
    "import json, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# from torchcrf import CRF\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0f5fd",
   "metadata": {},
   "source": [
    "# 基于机器学习的文本分类\n",
    "## 文本向量化\n",
    "\n",
    "- 将文本数据转换为 Bag-of-Words（词袋）特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3795681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # 简单空格分词，实际可扩展\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(texts, max_vocab_size=10000, min_freq=1):\n",
    "    # 统计词频，建立词表\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize(text))\n",
    "    # 过滤低频词，截断词表\n",
    "    vocab = [word for word, freq in counter.items() if freq >= min_freq]\n",
    "    vocab = sorted(vocab, key=lambda w: counter[w], reverse=True)[:max_vocab_size]\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    return word2idx\n",
    "\n",
    "def text_to_bow(text, word2idx):\n",
    "    vec = np.zeros(len(word2idx))\n",
    "    for w in tokenize(text):\n",
    "        if w in word2idx:\n",
    "            vec[word2idx[w]] += 1\n",
    "    return vec\n",
    "\n",
    "def texts_to_bow(texts, word2idx, mode='bow'):\n",
    "    bow = np.array([text_to_bow(text, word2idx) for text in texts])\n",
    "    # 返回原始的词袋模型（计数矩阵）\n",
    "    if mode == 'bow':\n",
    "        return bow\n",
    "    # 返回词频，对每行进行归一化\n",
    "    elif mode == 'tf':\n",
    "        return bow / np.maximum(1, bow.sum(axis=1, keepdims=True))\n",
    "    # 使用 TfidfTransformer 转换为 TF-IDF 特征\n",
    "    elif mode == 'tfidf':\n",
    "        transformer = TfidfTransformer()\n",
    "        return transformer.fit_transform(bow).toarray()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31569c",
   "metadata": {},
   "source": [
    "## 定义逻辑回归模型中常用的三个辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e1ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # 防止溢出\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# one-hot编码\n",
    "def one_hot(labels, num_classes, smooth=False, smoothing=0.1):\n",
    "    oh = np.zeros((len(labels), num_classes))\n",
    "    if smooth:\n",
    "        oh += smoothing / num_classes\n",
    "        for i, l in enumerate(labels):\n",
    "            oh[i, l] = 1 - smoothing + (smoothing / num_classes)\n",
    "    else:\n",
    "        for i, l in enumerate(labels):\n",
    "            oh[i, l] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d04d03",
   "metadata": {},
   "source": [
    "## 构建逻辑回归分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244c0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim, num_classes=2, lr=0.1, loss_type='ce'):\n",
    "        self.W = np.random.randn(input_dim, num_classes) * 0.01\n",
    "        self.b = np.zeros(num_classes)\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, X):\n",
    "        logits = X @ self.W + self.b\n",
    "        return softmax(logits)\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        probs = self.forward(X)\n",
    "        smooth = self.loss_type == 'ls'\n",
    "        y_onehot = one_hot(y, self.num_classes, smooth=smooth)\n",
    "        loss = -np.mean(np.sum(y_onehot * np.log(probs + 1e-15), axis=1))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        probs = self.forward(X)\n",
    "        smooth = self.loss_type == 'ls'\n",
    "        y_onehot = one_hot(y, self.num_classes, smooth=smooth)\n",
    "        dz = (probs - y_onehot) / m\n",
    "        dW = X.T @ dz\n",
    "        db = np.sum(dz, axis=0)\n",
    "        return dW, db\n",
    "\n",
    "    def update_params(self, dW, db):\n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=10, batch_size=64, shuffle=True):\n",
    "        m = X_train.shape[0]\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            if shuffle:\n",
    "                indices = np.random.permutation(m)\n",
    "            else:\n",
    "                indices = np.arange(m)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "            for start in range(0, m, batch_size):\n",
    "                end = start + batch_size\n",
    "                X_batch = X_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "                dW, db = self.backward(X_batch, y_batch)\n",
    "                self.update_params(dW, db)\n",
    "            acc = np.mean(self.predict(X_train) == y_train)\n",
    "            print(f\"Epoch {epoch}: acc={acc:.4f}, loss={self.compute_loss(X_train, y_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289231bf",
   "metadata": {},
   "source": [
    "## 运行程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fd3efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[实验] 特征=bow, 学习率=0.001, 损失函数=ce\n",
      "Epoch 1: acc=0.5099, loss=1.3925\n",
      "Epoch 2: acc=0.5102, loss=1.3236\n",
      "Epoch 3: acc=0.5123, loss=1.2911\n",
      "Epoch 4: acc=0.5142, loss=1.2724\n",
      "Epoch 5: acc=0.5155, loss=1.2602\n",
      "Epoch 6: acc=0.5167, loss=1.2516\n",
      "Epoch 7: acc=0.5177, loss=1.2450\n",
      "Epoch 8: acc=0.5184, loss=1.2397\n",
      "Epoch 9: acc=0.5195, loss=1.2354\n",
      "Epoch 10: acc=0.5205, loss=1.2317\n",
      "[输出] 已保存至 submission_bow_lr0.001_lossce.csv\n",
      "\n",
      "[实验] 特征=bow, 学习率=0.001, 损失函数=ls\n",
      "Epoch 1: acc=0.5099, loss=1.4326\n",
      "Epoch 2: acc=0.5101, loss=1.3769\n",
      "Epoch 3: acc=0.5121, loss=1.3509\n",
      "Epoch 4: acc=0.5139, loss=1.3361\n",
      "Epoch 5: acc=0.5154, loss=1.3265\n",
      "Epoch 6: acc=0.5163, loss=1.3198\n",
      "Epoch 7: acc=0.5175, loss=1.3147\n",
      "Epoch 8: acc=0.5185, loss=1.3107\n",
      "Epoch 9: acc=0.5192, loss=1.3074\n",
      "Epoch 10: acc=0.5202, loss=1.3046\n",
      "[输出] 已保存至 submission_bow_lr0.001_lossls.csv\n",
      "\n",
      "[实验] 特征=bow, 学习率=0.01, 损失函数=ce\n",
      "Epoch 1: acc=0.5204, loss=1.2317\n",
      "Epoch 2: acc=0.5254, loss=1.2110\n",
      "Epoch 3: acc=0.5286, loss=1.2004\n",
      "Epoch 4: acc=0.5314, loss=1.1927\n",
      "Epoch 5: acc=0.5333, loss=1.1864\n",
      "Epoch 6: acc=0.5357, loss=1.1808\n",
      "Epoch 7: acc=0.5375, loss=1.1757\n",
      "Epoch 8: acc=0.5378, loss=1.1711\n",
      "Epoch 9: acc=0.5396, loss=1.1668\n",
      "Epoch 10: acc=0.5412, loss=1.1627\n",
      "[输出] 已保存至 submission_bow_lr0.01_lossce.csv\n",
      "\n",
      "[实验] 特征=bow, 学习率=0.01, 损失函数=ls\n",
      "Epoch 1: acc=0.5201, loss=1.3047\n",
      "Epoch 2: acc=0.5252, loss=1.2897\n",
      "Epoch 3: acc=0.5290, loss=1.2821\n",
      "Epoch 4: acc=0.5306, loss=1.2763\n",
      "Epoch 5: acc=0.5328, loss=1.2713\n",
      "Epoch 6: acc=0.5345, loss=1.2669\n",
      "Epoch 7: acc=0.5366, loss=1.2629\n",
      "Epoch 8: acc=0.5382, loss=1.2592\n",
      "Epoch 9: acc=0.5391, loss=1.2558\n",
      "Epoch 10: acc=0.5407, loss=1.2526\n",
      "[输出] 已保存至 submission_bow_lr0.01_lossls.csv\n",
      "\n",
      "[实验] 特征=bow, 学习率=0.1, 损失函数=ce\n",
      "Epoch 1: acc=0.5398, loss=1.1630\n",
      "Epoch 2: acc=0.5509, loss=1.1319\n",
      "Epoch 3: acc=0.5571, loss=1.1100\n",
      "Epoch 4: acc=0.5630, loss=1.0932\n",
      "Epoch 5: acc=0.5722, loss=1.0780\n",
      "Epoch 6: acc=0.5725, loss=1.0670\n",
      "Epoch 7: acc=0.5819, loss=1.0553\n",
      "Epoch 8: acc=0.5819, loss=1.0461\n",
      "Epoch 9: acc=0.5898, loss=1.0378\n",
      "Epoch 10: acc=0.5934, loss=1.0304\n",
      "[输出] 已保存至 submission_bow_lr0.1_lossce.csv\n",
      "\n",
      "[实验] 特征=bow, 学习率=0.1, 损失函数=ls\n",
      "Epoch 1: acc=0.5405, loss=1.2535\n",
      "Epoch 2: acc=0.5522, loss=1.2289\n",
      "Epoch 3: acc=0.5580, loss=1.2115\n",
      "Epoch 4: acc=0.5677, loss=1.1981\n",
      "Epoch 5: acc=0.5709, loss=1.1868\n",
      "Epoch 6: acc=0.5781, loss=1.1776\n",
      "Epoch 7: acc=0.5777, loss=1.1691\n",
      "Epoch 8: acc=0.5821, loss=1.1619\n",
      "Epoch 9: acc=0.5854, loss=1.1555\n",
      "Epoch 10: acc=0.5877, loss=1.1498\n",
      "[输出] 已保存至 submission_bow_lr0.1_lossls.csv\n",
      "\n",
      "[实验] 特征=tf, 学习率=0.001, 损失函数=ce\n",
      "Epoch 1: acc=0.5099, loss=1.3982\n",
      "Epoch 2: acc=0.5099, loss=1.3317\n",
      "Epoch 3: acc=0.5099, loss=1.3084\n",
      "Epoch 4: acc=0.5099, loss=1.2981\n",
      "Epoch 5: acc=0.5099, loss=1.2925\n",
      "Epoch 6: acc=0.5099, loss=1.2891\n",
      "Epoch 7: acc=0.5099, loss=1.2869\n",
      "Epoch 8: acc=0.5099, loss=1.2854\n",
      "Epoch 9: acc=0.5099, loss=1.2844\n",
      "Epoch 10: acc=0.5099, loss=1.2837\n",
      "[输出] 已保存至 submission_tf_lr0.001_lossce.csv\n",
      "\n",
      "[实验] 特征=tf, 学习率=0.001, 损失函数=ls\n",
      "Epoch 1: acc=0.5099, loss=1.4376\n",
      "Epoch 2: acc=0.5099, loss=1.3833\n",
      "Epoch 3: acc=0.5099, loss=1.3648\n",
      "Epoch 4: acc=0.5099, loss=1.3571\n",
      "Epoch 5: acc=0.5099, loss=1.3532\n",
      "Epoch 6: acc=0.5099, loss=1.3510\n",
      "Epoch 7: acc=0.5099, loss=1.3496\n",
      "Epoch 8: acc=0.5099, loss=1.3488\n",
      "Epoch 9: acc=0.5099, loss=1.3482\n",
      "Epoch 10: acc=0.5099, loss=1.3478\n",
      "[输出] 已保存至 submission_tf_lr0.001_lossls.csv\n",
      "\n",
      "[实验] 特征=tf, 学习率=0.01, 损失函数=ce\n",
      "Epoch 1: acc=0.5099, loss=1.2836\n",
      "Epoch 2: acc=0.5099, loss=1.2809\n",
      "Epoch 3: acc=0.5099, loss=1.2794\n",
      "Epoch 4: acc=0.5099, loss=1.2782\n",
      "Epoch 5: acc=0.5099, loss=1.2769\n",
      "Epoch 6: acc=0.5099, loss=1.2759\n",
      "Epoch 7: acc=0.5099, loss=1.2748\n",
      "Epoch 8: acc=0.5099, loss=1.2738\n",
      "Epoch 9: acc=0.5099, loss=1.2729\n",
      "Epoch 10: acc=0.5099, loss=1.2720\n",
      "[输出] 已保存至 submission_tf_lr0.01_lossce.csv\n",
      "\n",
      "[实验] 特征=tf, 学习率=0.01, 损失函数=ls\n",
      "Epoch 1: acc=0.5099, loss=1.3477\n",
      "Epoch 2: acc=0.5099, loss=1.3460\n",
      "Epoch 3: acc=0.5099, loss=1.3449\n",
      "Epoch 4: acc=0.5099, loss=1.3438\n",
      "Epoch 5: acc=0.5099, loss=1.3428\n",
      "Epoch 6: acc=0.5099, loss=1.3419\n",
      "Epoch 7: acc=0.5099, loss=1.3411\n",
      "Epoch 8: acc=0.5099, loss=1.3403\n",
      "Epoch 9: acc=0.5099, loss=1.3396\n",
      "Epoch 10: acc=0.5099, loss=1.3388\n",
      "[输出] 已保存至 submission_tf_lr0.01_lossls.csv\n",
      "\n",
      "[实验] 特征=tf, 学习率=0.1, 损失函数=ce\n",
      "Epoch 1: acc=0.5099, loss=1.2722\n",
      "Epoch 2: acc=0.5084, loss=1.2653\n",
      "Epoch 3: acc=0.5070, loss=1.2597\n",
      "Epoch 4: acc=0.5049, loss=1.2554\n",
      "Epoch 5: acc=0.5065, loss=1.2513\n",
      "Epoch 6: acc=0.5044, loss=1.2478\n",
      "Epoch 7: acc=0.5048, loss=1.2445\n",
      "Epoch 8: acc=0.5049, loss=1.2417\n",
      "Epoch 9: acc=0.5074, loss=1.2388\n",
      "Epoch 10: acc=0.5062, loss=1.2363\n",
      "[输出] 已保存至 submission_tf_lr0.1_lossce.csv\n",
      "\n",
      "[实验] 特征=tf, 学习率=0.1, 损失函数=ls\n",
      "Epoch 1: acc=0.5099, loss=1.3397\n",
      "Epoch 2: acc=0.5087, loss=1.3334\n",
      "Epoch 3: acc=0.5070, loss=1.3291\n",
      "Epoch 4: acc=0.5071, loss=1.3254\n",
      "Epoch 5: acc=0.5065, loss=1.3223\n",
      "Epoch 6: acc=0.5045, loss=1.3195\n",
      "Epoch 7: acc=0.5047, loss=1.3170\n",
      "Epoch 8: acc=0.5069, loss=1.3147\n",
      "Epoch 9: acc=0.5052, loss=1.3124\n",
      "Epoch 10: acc=0.5060, loss=1.3104\n",
      "[输出] 已保存至 submission_tf_lr0.1_lossls.csv\n",
      "\n",
      "[实验] 特征=tfidf, 学习率=0.001, 损失函数=ce\n",
      "Epoch 1: acc=0.5099, loss=1.3981\n",
      "Epoch 2: acc=0.5099, loss=1.3313\n",
      "Epoch 3: acc=0.5099, loss=1.3079\n",
      "Epoch 4: acc=0.5099, loss=1.2972\n",
      "Epoch 5: acc=0.5099, loss=1.2913\n",
      "Epoch 6: acc=0.5099, loss=1.2875\n",
      "Epoch 7: acc=0.5099, loss=1.2850\n",
      "Epoch 8: acc=0.5099, loss=1.2832\n",
      "Epoch 9: acc=0.5099, loss=1.2818\n",
      "Epoch 10: acc=0.5099, loss=1.2807\n",
      "[输出] 已保存至 submission_tfidf_lr0.001_lossce.csv\n",
      "\n",
      "[实验] 特征=tfidf, 学习率=0.001, 损失函数=ls\n",
      "Epoch 1: acc=0.5099, loss=1.4379\n",
      "Epoch 2: acc=0.5099, loss=1.3836\n",
      "Epoch 3: acc=0.5099, loss=1.3650\n",
      "Epoch 4: acc=0.5099, loss=1.3570\n",
      "Epoch 5: acc=0.5099, loss=1.3528\n",
      "Epoch 6: acc=0.5099, loss=1.3503\n",
      "Epoch 7: acc=0.5099, loss=1.3486\n",
      "Epoch 8: acc=0.5099, loss=1.3474\n",
      "Epoch 9: acc=0.5099, loss=1.3465\n",
      "Epoch 10: acc=0.5099, loss=1.3458\n",
      "[输出] 已保存至 submission_tfidf_lr0.001_lossls.csv\n",
      "\n",
      "[实验] 特征=tfidf, 学习率=0.01, 损失函数=ce\n",
      "Epoch 1: acc=0.5099, loss=1.2807\n",
      "Epoch 2: acc=0.5099, loss=1.2742\n",
      "Epoch 3: acc=0.5099, loss=1.2695\n",
      "Epoch 4: acc=0.5099, loss=1.2651\n",
      "Epoch 5: acc=0.5100, loss=1.2611\n",
      "Epoch 6: acc=0.5100, loss=1.2574\n",
      "Epoch 7: acc=0.5097, loss=1.2539\n",
      "Epoch 8: acc=0.5096, loss=1.2507\n",
      "Epoch 9: acc=0.5096, loss=1.2476\n",
      "Epoch 10: acc=0.5097, loss=1.2448\n",
      "[输出] 已保存至 submission_tfidf_lr0.01_lossce.csv\n",
      "\n",
      "[实验] 特征=tfidf, 学习率=0.01, 损失函数=ls\n",
      "Epoch 1: acc=0.5099, loss=1.3457\n",
      "Epoch 2: acc=0.5099, loss=1.3410\n",
      "Epoch 3: acc=0.5099, loss=1.3372\n",
      "Epoch 4: acc=0.5099, loss=1.3336\n",
      "Epoch 5: acc=0.5099, loss=1.3303\n",
      "Epoch 6: acc=0.5100, loss=1.3273\n",
      "Epoch 7: acc=0.5099, loss=1.3244\n",
      "Epoch 8: acc=0.5097, loss=1.3218\n",
      "Epoch 9: acc=0.5096, loss=1.3193\n",
      "Epoch 10: acc=0.5096, loss=1.3170\n",
      "[输出] 已保存至 submission_tfidf_lr0.01_lossls.csv\n",
      "\n",
      "[实验] 特征=tfidf, 学习率=0.1, 损失函数=ce\n",
      "Epoch 1: acc=0.5088, loss=1.2456\n",
      "Epoch 2: acc=0.5083, loss=1.2227\n",
      "Epoch 3: acc=0.5132, loss=1.2074\n",
      "Epoch 4: acc=0.5175, loss=1.1936\n",
      "Epoch 5: acc=0.5215, loss=1.1826\n",
      "Epoch 6: acc=0.5263, loss=1.1727\n",
      "Epoch 7: acc=0.5294, loss=1.1636\n",
      "Epoch 8: acc=0.5326, loss=1.1555\n",
      "Epoch 9: acc=0.5393, loss=1.1481\n",
      "Epoch 10: acc=0.5412, loss=1.1410\n",
      "[输出] 已保存至 submission_tfidf_lr0.1_lossce.csv\n",
      "\n",
      "[实验] 特征=tfidf, 学习率=0.1, 损失函数=ls\n",
      "Epoch 1: acc=0.5096, loss=1.3170\n",
      "Epoch 2: acc=0.5106, loss=1.2993\n",
      "Epoch 3: acc=0.5113, loss=1.2866\n",
      "Epoch 4: acc=0.5159, loss=1.2760\n",
      "Epoch 5: acc=0.5204, loss=1.2672\n",
      "Epoch 6: acc=0.5258, loss=1.2596\n",
      "Epoch 7: acc=0.5303, loss=1.2526\n",
      "Epoch 8: acc=0.5325, loss=1.2460\n",
      "Epoch 9: acc=0.5365, loss=1.2401\n",
      "Epoch 10: acc=0.5403, loss=1.2350\n",
      "[输出] 已保存至 submission_tfidf_lr0.1_lossls.csv\n",
      "\n",
      "=== 所有组合结果 ===\n",
      "bow + lr=0.001 + loss=ce: acc=0.5205\n",
      "bow + lr=0.001 + loss=ls: acc=0.5202\n",
      "bow + lr=0.01 + loss=ce: acc=0.5412\n",
      "bow + lr=0.01 + loss=ls: acc=0.5407\n",
      "bow + lr=0.1 + loss=ce: acc=0.5934\n",
      "bow + lr=0.1 + loss=ls: acc=0.5877\n",
      "tf + lr=0.001 + loss=ce: acc=0.5099\n",
      "tf + lr=0.001 + loss=ls: acc=0.5099\n",
      "tf + lr=0.01 + loss=ce: acc=0.5099\n",
      "tf + lr=0.01 + loss=ls: acc=0.5099\n",
      "tf + lr=0.1 + loss=ce: acc=0.5062\n",
      "tf + lr=0.1 + loss=ls: acc=0.5060\n",
      "tfidf + lr=0.001 + loss=ce: acc=0.5099\n",
      "tfidf + lr=0.001 + loss=ls: acc=0.5099\n",
      "tfidf + lr=0.01 + loss=ce: acc=0.5097\n",
      "tfidf + lr=0.01 + loss=ls: acc=0.5096\n",
      "tfidf + lr=0.1 + loss=ce: acc=0.5412\n",
      "tfidf + lr=0.1 + loss=ls: acc=0.5403\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(feature_mode='bow', lr=0.1, loss_type='ce', vocab_size=5000):\n",
    "    print(f\"\\n[实验] 特征={feature_mode}, 学习率={lr}, 损失函数={loss_type}\")\n",
    "    Train = pd.read_csv('D:/Data/master/train.tsv', sep='\\t')\n",
    "    Test = pd.read_csv('D:/Data/master/test.tsv', sep='\\t')\n",
    "    train_texts = Train['Phrase'].astype(str).values\n",
    "    train_labels = Train['Sentiment'].values\n",
    "    test_texts = Test['Phrase'].astype(str).values\n",
    "\n",
    "    word2idx = build_vocab(train_texts, max_vocab_size=vocab_size)\n",
    "    X_train = texts_to_bow(train_texts, word2idx, mode=feature_mode)\n",
    "    X_test = texts_to_bow(test_texts, word2idx, mode=feature_mode)\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        input_dim=X_train.shape[1],\n",
    "        num_classes=5,\n",
    "        lr=lr,\n",
    "        loss_type=loss_type\n",
    "    )\n",
    "    model.fit(X_train, train_labels, epochs=10, batch_size=64)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    output_df = pd.DataFrame({\n",
    "        'PhraseId': Test['PhraseId'],\n",
    "        'Sentiment': y_pred\n",
    "    })\n",
    "    filename = f'submission_{feature_mode}_lr{lr}_loss{loss_type}.csv'\n",
    "    output_df.to_csv(filename, index=False)\n",
    "    print(f\"[输出] 已保存至 {filename}\")\n",
    "    return np.mean(model.predict(X_train) == train_labels)\n",
    "\n",
    "'''主程序'''\n",
    "if __name__ == '__main__':\n",
    "    feature_modes = ['bow', 'tf', 'tfidf']\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    loss_types = ['ce', 'ls']  # ce: cross entropy, ls: label smoothing\n",
    "\n",
    "    results = {}\n",
    "    for feat in feature_modes:\n",
    "        for lr in learning_rates:\n",
    "            for loss in loss_types:\n",
    "                acc = run_experiment(feature_mode=feat, lr=lr, loss_type=loss)\n",
    "                results[(feat, lr, loss)] = acc\n",
    "\n",
    "    print(\"\\n=== 所有组合结果 ===\")\n",
    "    for (feat, lr, loss), acc in results.items():\n",
    "        print(f\"{feat} + lr={lr} + loss={loss}: acc={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f0c3c",
   "metadata": {},
   "source": [
    "# 基于深度学习的文本分类\n",
    "## 文本向量化(深度学习模式)\n",
    "\n",
    "- 使用词嵌入序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0f78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(texts, max_vocab_size=10000, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize(text))\n",
    "    vocab = [word for word, freq in counter.items() if freq >= min_freq]\n",
    "    vocab = vocab[:max_vocab_size - 2]\n",
    "    word2idx = {word: idx + 2 for idx, word in enumerate(vocab)}\n",
    "    word2idx['<PAD>'] = 0\n",
    "    word2idx['<UNK>'] = 1\n",
    "    return word2idx\n",
    "# 将一条文本编码成固定长度的索引序列\n",
    "def encode(text, word2idx, max_len=50):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [word2idx.get(w, 0) for w in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [0] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "def get_feature_tensor(texts, word2idx, mode='index', max_len=50):\n",
    "    if mode == 'index':  # 用于 CNN/RNN\n",
    "        return [torch.tensor(encode(t, word2idx, max_len)) for t in texts]\n",
    "    else:  # bow / tf / tfidf\n",
    "        bow = np.zeros((len(texts), len(word2idx)))\n",
    "        for i, text in enumerate(texts):\n",
    "            for w in tokenize(text):\n",
    "                if w in word2idx:\n",
    "                    bow[i][word2idx[w]] += 1\n",
    "        if mode == 'tf':\n",
    "            bow = bow / np.maximum(1, bow.sum(axis=1, keepdims=True))\n",
    "        elif mode == 'tfidf':\n",
    "            transformer = TfidfTransformer()\n",
    "            bow = transformer.fit_transform(bow).toarray()\n",
    "        return [torch.tensor(row, dtype=torch.float32) for row in bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af47cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''封装输入文本和标签，供DataLoader使用'''\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea5450",
   "metadata": {},
   "source": [
    "## 构建深度学习模型\n",
    "- 卷积神经网络（CNN）、循环神经网络（RNN） 和 线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcec1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, kernel_sizes=[3,4,5], num_filters=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(embed_dim, num_filters, k) for k in kernel_sizes])\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        x = [F.relu(conv(x)).max(dim=2)[0] for conv in self.convs]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class RNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n[-1])\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf176850",
   "metadata": {},
   "source": [
    "> **损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cceef291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fn(loss_type='ce', smoothing=0.1, num_classes=5):\n",
    "    # 普通交叉熵损失\n",
    "    if loss_type == 'ce':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    # 标签平滑交叉熵\n",
    "    elif loss_type == 'ls':\n",
    "        def loss_fn(pred, target):\n",
    "            one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)\n",
    "            one_hot = one_hot * (1 - smoothing) + smoothing / num_classes\n",
    "            log_prob = F.log_softmax(pred, dim=1)\n",
    "            return -(one_hot * log_prob).sum(dim=1).mean()\n",
    "        return loss_fn\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss type: {loss_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9a0a5",
   "metadata": {},
   "source": [
    "> **训练与评估**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda37299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, lr, loss_type, device):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = get_loss_fn(loss_type, num_classes=5)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        correct, total_loss = 0, 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "        acc = correct / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch}: Loss={total_loss/len(train_loader.dataset):.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "    return evaluate(model, val_loader, device)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798305d3",
   "metadata": {},
   "source": [
    "## 主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e31a6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model_type='cnn', feature_type='index', loss_type='ce', lr=0.001, embed_dim=100):\n",
    "    print(f\"\\n>>> 模型={model_type}, 特征={feature_type}, loss={loss_type}, lr={lr}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Train = pd.read_csv('D:/Data/master/train.tsv', sep='\\t')\n",
    "#     Test = pd.read_csv('D:/Data/master/test.tsv', sep='\\t')\n",
    "    train_texts, train_labels = Train['Phrase'].astype(str).tolist(), Train['Sentiment'].tolist()\n",
    "#     val_texts, val_labels = Test['Phrase'].astype(str).tolist(), Test['Sentiment'].tolist()\n",
    "    # ---------------- 构建词表和特征向量 ----------------\n",
    "    word2idx = build_vocab(train_texts, max_vocab_size=10000)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "    train_inputs = get_feature_tensor(X_train, word2idx, mode=feature_type)\n",
    "    val_inputs = get_feature_tensor(X_val, word2idx, mode=feature_type)\n",
    "    train_dataset = TextDataset(train_inputs, y_train)\n",
    "    val_dataset = TextDataset(val_inputs, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    # ---------------- 构建模型 ----------------\n",
    "    if feature_type == 'index':\n",
    "        vocab_size = len(word2idx)\n",
    "        if model_type == 'cnn':\n",
    "            model = CNNTextClassifier(vocab_size, embed_dim, num_classes=5)\n",
    "        elif model_type == 'rnn':\n",
    "            model = RNNTextClassifier(vocab_size, embed_dim, num_classes=5)\n",
    "        else:\n",
    "            raise ValueError(\"Only CNN/RNN support index input.\")\n",
    "    else:\n",
    "        model = LinearClassifier(input_dim=len(word2idx), num_classes=5)\n",
    "\n",
    "    acc = train_model(model, train_loader, val_loader, epochs=10, lr=lr, loss_type=loss_type, device=device)\n",
    "    print(f\"==> Final Val Acc: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96be557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> 模型=cnn, 特征=index, loss=ce, lr=0.001\n",
      "Epoch 1: Loss=1.0817, Acc=0.5730\n",
      "Epoch 2: Loss=0.8626, Acc=0.6551\n",
      "Epoch 3: Loss=0.7662, Acc=0.6930\n",
      "Epoch 4: Loss=0.7020, Acc=0.7185\n",
      "Epoch 5: Loss=0.6521, Acc=0.7376\n",
      "Epoch 6: Loss=0.6084, Acc=0.7552\n",
      "Epoch 7: Loss=0.5745, Acc=0.7700\n",
      "Epoch 8: Loss=0.5452, Acc=0.7838\n",
      "Epoch 9: Loss=0.5111, Acc=0.7974\n",
      "Epoch 10: Loss=0.4871, Acc=0.8075\n",
      "==> Final Val Acc: 0.6337\n",
      "\n",
      ">>> 模型=cnn, 特征=index, loss=ce, lr=0.01\n",
      "Epoch 1: Loss=1.1314, Acc=0.5686\n",
      "Epoch 2: Loss=0.9663, Acc=0.6248\n",
      "Epoch 3: Loss=0.9188, Acc=0.6441\n",
      "Epoch 4: Loss=0.8841, Acc=0.6571\n",
      "Epoch 5: Loss=0.8575, Acc=0.6675\n",
      "Epoch 6: Loss=0.8493, Acc=0.6745\n",
      "Epoch 7: Loss=0.8268, Acc=0.6804\n",
      "Epoch 8: Loss=0.8240, Acc=0.6840\n",
      "Epoch 9: Loss=0.8060, Acc=0.6881\n",
      "Epoch 10: Loss=0.7994, Acc=0.6931\n",
      "==> Final Val Acc: 0.6396\n",
      "\n",
      ">>> 模型=cnn, 特征=index, loss=ls, lr=0.001\n",
      "Epoch 1: Loss=1.2005, Acc=0.5716\n",
      "Epoch 2: Loss=1.0442, Acc=0.6534\n",
      "Epoch 3: Loss=0.9751, Acc=0.6909\n",
      "Epoch 4: Loss=0.9306, Acc=0.7183\n",
      "Epoch 5: Loss=0.8972, Acc=0.7364\n",
      "Epoch 6: Loss=0.8672, Acc=0.7524\n",
      "Epoch 7: Loss=0.8449, Acc=0.7651\n",
      "Epoch 8: Loss=0.8222, Acc=0.7788\n",
      "Epoch 9: Loss=0.8036, Acc=0.7907\n",
      "Epoch 10: Loss=0.7843, Acc=0.8007\n",
      "==> Final Val Acc: 0.6315\n",
      "\n",
      ">>> 模型=cnn, 特征=index, loss=ls, lr=0.01\n",
      "Epoch 1: Loss=1.2373, Acc=0.5678\n",
      "Epoch 2: Loss=1.1118, Acc=0.6264\n",
      "Epoch 3: Loss=1.0777, Acc=0.6447\n",
      "Epoch 4: Loss=1.0552, Acc=0.6570\n",
      "Epoch 5: Loss=1.0371, Acc=0.6663\n",
      "Epoch 6: Loss=1.0246, Acc=0.6724\n",
      "Epoch 7: Loss=1.0140, Acc=0.6780\n",
      "Epoch 8: Loss=1.0151, Acc=0.6811\n",
      "Epoch 9: Loss=1.0010, Acc=0.6854\n",
      "Epoch 10: Loss=0.9923, Acc=0.6875\n",
      "==> Final Val Acc: 0.6365\n",
      "\n",
      ">>> 模型=rnn, 特征=index, loss=ce, lr=0.001\n",
      "Epoch 1: Loss=1.1697, Acc=0.5367\n",
      "Epoch 2: Loss=1.0387, Acc=0.5887\n",
      "Epoch 3: Loss=1.0100, Acc=0.6044\n",
      "Epoch 4: Loss=0.9312, Acc=0.6387\n",
      "Epoch 5: Loss=0.8661, Acc=0.6648\n",
      "Epoch 6: Loss=0.8169, Acc=0.6836\n",
      "Epoch 7: Loss=0.7739, Acc=0.7009\n",
      "Epoch 8: Loss=0.7361, Acc=0.7153\n",
      "Epoch 9: Loss=0.7029, Acc=0.7273\n",
      "Epoch 10: Loss=0.6729, Acc=0.7398\n",
      "==> Final Val Acc: 0.6548\n",
      "\n",
      ">>> 模型=rnn, 特征=index, loss=ce, lr=0.01\n",
      "Epoch 1: Loss=1.2744, Acc=0.5120\n",
      "Epoch 2: Loss=1.1483, Acc=0.5406\n",
      "Epoch 3: Loss=1.0508, Acc=0.5881\n",
      "Epoch 4: Loss=0.9928, Acc=0.6097\n",
      "Epoch 5: Loss=0.9590, Acc=0.6226\n",
      "Epoch 6: Loss=0.9720, Acc=0.6192\n",
      "Epoch 7: Loss=1.2903, Acc=0.5105\n",
      "Epoch 8: Loss=1.2572, Acc=0.5149\n",
      "Epoch 9: Loss=1.2619, Acc=0.5150\n",
      "Epoch 10: Loss=1.2612, Acc=0.5161\n",
      "==> Final Val Acc: 0.5064\n",
      "\n",
      ">>> 模型=rnn, 特征=index, loss=ls, lr=0.001\n",
      "Epoch 1: Loss=1.2618, Acc=0.5363\n",
      "Epoch 2: Loss=1.1603, Acc=0.5898\n",
      "Epoch 3: Loss=1.0914, Acc=0.6298\n",
      "Epoch 4: Loss=1.0423, Acc=0.6584\n",
      "Epoch 5: Loss=1.0071, Acc=0.6777\n",
      "Epoch 6: Loss=0.9804, Acc=0.6932\n",
      "Epoch 7: Loss=0.9703, Acc=0.6998\n",
      "Epoch 8: Loss=0.9413, Acc=0.7163\n",
      "Epoch 9: Loss=0.9187, Acc=0.7273\n",
      "Epoch 10: Loss=0.8983, Acc=0.7371\n",
      "==> Final Val Acc: 0.6626\n",
      "\n",
      ">>> 模型=rnn, 特征=index, loss=ls, lr=0.01\n",
      "Epoch 1: Loss=1.3387, Acc=0.5150\n",
      "Epoch 2: Loss=1.1901, Acc=0.5733\n",
      "Epoch 3: Loss=1.0998, Acc=0.6188\n",
      "Epoch 4: Loss=1.0631, Acc=0.6378\n",
      "Epoch 5: Loss=1.0496, Acc=0.6468\n",
      "Epoch 6: Loss=1.0359, Acc=0.6520\n",
      "Epoch 7: Loss=1.0268, Acc=0.6588\n",
      "Epoch 8: Loss=1.0168, Acc=0.6626\n",
      "Epoch 9: Loss=1.0129, Acc=0.6647\n",
      "Epoch 10: Loss=1.0072, Acc=0.6696\n",
      "==> Final Val Acc: 0.6271\n",
      "\n",
      ">>> 模型=linear, 特征=bow, loss=ce, lr=0.001\n",
      "Epoch 1: Loss=1.2544, Acc=0.5392\n",
      "Epoch 2: Loss=1.0416, Acc=0.5814\n",
      "Epoch 3: Loss=0.9744, Acc=0.6064\n",
      "Epoch 4: Loss=0.9333, Acc=0.6227\n",
      "Epoch 5: Loss=0.9039, Acc=0.6360\n",
      "Epoch 6: Loss=0.8814, Acc=0.6457\n",
      "Epoch 7: Loss=0.8635, Acc=0.6536\n",
      "Epoch 8: Loss=0.8488, Acc=0.6601\n",
      "Epoch 9: Loss=0.8368, Acc=0.6653\n",
      "Epoch 10: Loss=0.8263, Acc=0.6698\n",
      "==> Final Val Acc: 0.6302\n",
      "\n",
      ">>> 模型=linear, 特征=bow, loss=ce, lr=0.01\n",
      "Epoch 1: Loss=1.0497, Acc=0.5947\n",
      "Epoch 2: Loss=0.8928, Acc=0.6503\n",
      "Epoch 3: Loss=0.8544, Acc=0.6635\n",
      "Epoch 4: Loss=0.8345, Acc=0.6707\n",
      "Epoch 5: Loss=0.8220, Acc=0.6741\n",
      "Epoch 6: Loss=0.8138, Acc=0.6777\n",
      "Epoch 7: Loss=0.8078, Acc=0.6804\n",
      "Epoch 8: Loss=0.8034, Acc=0.6818\n",
      "Epoch 9: Loss=0.8003, Acc=0.6827\n",
      "Epoch 10: Loss=0.7973, Acc=0.6835\n",
      "==> Final Val Acc: 0.6378\n",
      "\n",
      ">>> 模型=linear, 特征=bow, loss=ls, lr=0.001\n",
      "Epoch 1: Loss=1.3099, Acc=0.5411\n",
      "Epoch 2: Loss=1.1471, Acc=0.5842\n",
      "Epoch 3: Loss=1.0990, Acc=0.6099\n",
      "Epoch 4: Loss=1.0702, Acc=0.6271\n",
      "Epoch 5: Loss=1.0506, Acc=0.6391\n",
      "Epoch 6: Loss=1.0361, Acc=0.6488\n",
      "Epoch 7: Loss=1.0249, Acc=0.6561\n",
      "Epoch 8: Loss=1.0159, Acc=0.6623\n",
      "Epoch 9: Loss=1.0087, Acc=0.6671\n",
      "Epoch 10: Loss=1.0026, Acc=0.6715\n",
      "==> Final Val Acc: 0.6316\n",
      "\n",
      ">>> 模型=linear, 特征=bow, loss=ls, lr=0.01\n",
      "Epoch 1: Loss=1.1713, Acc=0.5960\n",
      "Epoch 2: Loss=1.0688, Acc=0.6458\n",
      "Epoch 3: Loss=1.0484, Acc=0.6574\n",
      "Epoch 4: Loss=1.0402, Acc=0.6657\n",
      "Epoch 5: Loss=1.0347, Acc=0.6691\n",
      "Epoch 6: Loss=1.0323, Acc=0.6688\n",
      "Epoch 7: Loss=1.0300, Acc=0.6708\n",
      "Epoch 8: Loss=1.0287, Acc=0.6718\n",
      "Epoch 9: Loss=1.0277, Acc=0.6739\n",
      "Epoch 10: Loss=1.0274, Acc=0.6739\n",
      "==> Final Val Acc: 0.6291\n",
      "\n",
      ">>> 模型=linear, 特征=tf, loss=ce, lr=0.001\n",
      "Epoch 1: Loss=1.3263, Acc=0.5105\n",
      "Epoch 2: Loss=1.2202, Acc=0.5132\n",
      "Epoch 3: Loss=1.1838, Acc=0.5175\n",
      "Epoch 4: Loss=1.1550, Acc=0.5240\n",
      "Epoch 5: Loss=1.1311, Acc=0.5313\n",
      "Epoch 6: Loss=1.1105, Acc=0.5380\n",
      "Epoch 7: Loss=1.0926, Acc=0.5445\n",
      "Epoch 8: Loss=1.0769, Acc=0.5506\n",
      "Epoch 9: Loss=1.0628, Acc=0.5559\n",
      "Epoch 10: Loss=1.0502, Acc=0.5614\n",
      "==> Final Val Acc: 0.5427\n",
      "\n",
      ">>> 模型=linear, 特征=tf, loss=ce, lr=0.01\n",
      "Epoch 1: Loss=1.1672, Acc=0.5292\n",
      "Epoch 2: Loss=1.0334, Acc=0.5683\n",
      "Epoch 3: Loss=0.9794, Acc=0.5876\n",
      "Epoch 4: Loss=0.9488, Acc=0.5984\n",
      "Epoch 5: Loss=0.9287, Acc=0.6042\n",
      "Epoch 6: Loss=0.9147, Acc=0.6099\n",
      "Epoch 7: Loss=0.9042, Acc=0.6131\n",
      "Epoch 8: Loss=0.8963, Acc=0.6166\n",
      "Epoch 9: Loss=0.8899, Acc=0.6181\n",
      "Epoch 10: Loss=0.8849, Acc=0.6194\n",
      "==> Final Val Acc: 0.5706\n",
      "\n",
      ">>> 模型=linear, 特征=tf, loss=ls, lr=0.001\n",
      "Epoch 1: Loss=1.3734, Acc=0.5119\n",
      "Epoch 2: Loss=1.2919, Acc=0.5138\n",
      "Epoch 3: Loss=1.2616, Acc=0.5184\n",
      "Epoch 4: Loss=1.2380, Acc=0.5257\n",
      "Epoch 5: Loss=1.2186, Acc=0.5339\n",
      "Epoch 6: Loss=1.2023, Acc=0.5411\n",
      "Epoch 7: Loss=1.1883, Acc=0.5479\n",
      "Epoch 8: Loss=1.1763, Acc=0.5546\n",
      "Epoch 9: Loss=1.1657, Acc=0.5599\n",
      "Epoch 10: Loss=1.1564, Acc=0.5653\n",
      "==> Final Val Acc: 0.5442\n",
      "\n",
      ">>> 模型=linear, 特征=tf, loss=ls, lr=0.01\n",
      "Epoch 1: Loss=1.2505, Acc=0.5302\n",
      "Epoch 2: Loss=1.1490, Acc=0.5711\n",
      "Epoch 3: Loss=1.1144, Acc=0.5890\n",
      "Epoch 4: Loss=1.0977, Acc=0.5993\n",
      "Epoch 5: Loss=1.0881, Acc=0.6045\n",
      "Epoch 6: Loss=1.0819, Acc=0.6093\n",
      "Epoch 7: Loss=1.0778, Acc=0.6121\n",
      "Epoch 8: Loss=1.0748, Acc=0.6151\n",
      "Epoch 9: Loss=1.0727, Acc=0.6159\n",
      "Epoch 10: Loss=1.0711, Acc=0.6179\n",
      "==> Final Val Acc: 0.5720\n",
      "\n",
      ">>> 模型=linear, 特征=tfidf, loss=ce, lr=0.001\n",
      "Epoch 1: Loss=1.3039, Acc=0.5119\n",
      "Epoch 2: Loss=1.1389, Acc=0.5339\n",
      "Epoch 3: Loss=1.0659, Acc=0.5615\n",
      "Epoch 4: Loss=1.0159, Acc=0.5826\n",
      "Epoch 5: Loss=0.9786, Acc=0.5993\n",
      "Epoch 6: Loss=0.9493, Acc=0.6138\n",
      "Epoch 7: Loss=0.9257, Acc=0.6249\n",
      "Epoch 8: Loss=0.9061, Acc=0.6340\n",
      "Epoch 9: Loss=0.8896, Acc=0.6411\n",
      "Epoch 10: Loss=0.8755, Acc=0.6474\n",
      "==> Final Val Acc: 0.6168\n",
      "\n",
      ">>> 模型=linear, 特征=tfidf, loss=ce, lr=0.01\n",
      "Epoch 1: Loss=1.0671, Acc=0.5761\n",
      "Epoch 2: Loss=0.8855, Acc=0.6440\n",
      "Epoch 3: Loss=0.8339, Acc=0.6634\n",
      "Epoch 4: Loss=0.8074, Acc=0.6735\n",
      "Epoch 5: Loss=0.7906, Acc=0.6778\n",
      "Epoch 6: Loss=0.7796, Acc=0.6822\n",
      "Epoch 7: Loss=0.7719, Acc=0.6854\n",
      "Epoch 8: Loss=0.7658, Acc=0.6873\n",
      "Epoch 9: Loss=0.7613, Acc=0.6895\n",
      "Epoch 10: Loss=0.7578, Acc=0.6896\n",
      "==> Final Val Acc: 0.6375\n",
      "\n",
      ">>> 模型=linear, 特征=tfidf, loss=ls, lr=0.001\n",
      "Epoch 1: Loss=1.3535, Acc=0.5132\n",
      "Epoch 2: Loss=1.2204, Acc=0.5374\n",
      "Epoch 3: Loss=1.1629, Acc=0.5670\n",
      "Epoch 4: Loss=1.1249, Acc=0.5888\n",
      "Epoch 5: Loss=1.0975, Acc=0.6061\n",
      "Epoch 6: Loss=1.0767, Acc=0.6196\n",
      "Epoch 7: Loss=1.0605, Acc=0.6311\n",
      "Epoch 8: Loss=1.0475, Acc=0.6399\n",
      "Epoch 9: Loss=1.0368, Acc=0.6473\n",
      "Epoch 10: Loss=1.0278, Acc=0.6529\n",
      "==> Final Val Acc: 0.6201\n",
      "\n",
      ">>> 模型=linear, 特征=tfidf, loss=ls, lr=0.01\n",
      "Epoch 1: Loss=1.1739, Acc=0.5794\n",
      "Epoch 2: Loss=1.0455, Acc=0.6464\n",
      "Epoch 3: Loss=1.0160, Acc=0.6631\n",
      "Epoch 4: Loss=1.0027, Acc=0.6718\n",
      "Epoch 5: Loss=0.9959, Acc=0.6745\n",
      "Epoch 6: Loss=0.9915, Acc=0.6777\n",
      "Epoch 7: Loss=0.9892, Acc=0.6789\n",
      "Epoch 8: Loss=0.9873, Acc=0.6814\n",
      "Epoch 9: Loss=0.9859, Acc=0.6817\n",
      "Epoch 10: Loss=0.9852, Acc=0.6825\n",
      "==> Final Val Acc: 0.6347\n",
      "\n",
      "=== 所有实验结果 ===\n",
      "('cnn', 'index', 'ce', 0.001): acc=0.6337\n",
      "('cnn', 'index', 'ce', 0.01): acc=0.6396\n",
      "('cnn', 'index', 'ls', 0.001): acc=0.6315\n",
      "('cnn', 'index', 'ls', 0.01): acc=0.6365\n",
      "('rnn', 'index', 'ce', 0.001): acc=0.6548\n",
      "('rnn', 'index', 'ce', 0.01): acc=0.5064\n",
      "('rnn', 'index', 'ls', 0.001): acc=0.6626\n",
      "('rnn', 'index', 'ls', 0.01): acc=0.6271\n",
      "('linear', 'bow', 'ce', 0.001): acc=0.6302\n",
      "('linear', 'bow', 'ce', 0.01): acc=0.6378\n",
      "('linear', 'bow', 'ls', 0.001): acc=0.6316\n",
      "('linear', 'bow', 'ls', 0.01): acc=0.6291\n",
      "('linear', 'tf', 'ce', 0.001): acc=0.5427\n",
      "('linear', 'tf', 'ce', 0.01): acc=0.5706\n",
      "('linear', 'tf', 'ls', 0.001): acc=0.5442\n",
      "('linear', 'tf', 'ls', 0.01): acc=0.5720\n",
      "('linear', 'tfidf', 'ce', 0.001): acc=0.6168\n",
      "('linear', 'tfidf', 'ce', 0.01): acc=0.6375\n",
      "('linear', 'tfidf', 'ls', 0.001): acc=0.6201\n",
      "('linear', 'tfidf', 'ls', 0.01): acc=0.6347\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model_types = ['cnn', 'rnn']  # 仅 index 特征可选\n",
    "    feature_types = ['index', 'bow', 'tf', 'tfidf']\n",
    "    loss_types = ['ce', 'ls']\n",
    "    learning_rates = [0.001, 0.01]\n",
    "\n",
    "    results = {}\n",
    "    for feat in feature_types:\n",
    "        applicable_models = model_types if feat == 'index' else ['linear']\n",
    "        for model_type in applicable_models:\n",
    "            for loss in loss_types:\n",
    "                for lr in learning_rates:\n",
    "                    key = (model_type, feat, loss, lr)\n",
    "                    acc = run_experiment(model_type=model_type, feature_type=feat, loss_type=loss, lr=lr)\n",
    "                    results[key] = acc\n",
    "\n",
    "    print(\"\\n=== 所有实验结果 ===\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: acc={v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095b79d",
   "metadata": {},
   "source": [
    "# 基于注意力机制的文本匹配\n",
    "## 数据集的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff3721f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本转为小写并按空格切分成单词\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "# 统计词频，构建一个词到索引的字典\n",
    "def build_vocab(sent_pairs, max_vocab=10000, min_freq=1):\n",
    "    cnt = Counter()\n",
    "    for s1, s2, _ in sent_pairs:\n",
    "        cnt.update(tokenize(s1))\n",
    "        cnt.update(tokenize(s2))\n",
    "    vocab = [w for w, freq in cnt.items() if freq >= min_freq][:max_vocab-2]\n",
    "    word2idx = {w: i+2 for i, w in enumerate(vocab)}\n",
    "    word2idx[\"<PAD>\"], word2idx[\"<UNK>\"] = 0, 1\n",
    "    return word2idx\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pairs, word2idx, max_len=50):\n",
    "        self.data = pairs\n",
    "        self.w2i = word2idx\n",
    "        self.max_len = max_len\n",
    "    def encode(self, s):\n",
    "        tok = tokenize(s)[:self.max_len]\n",
    "        ids = [self.w2i.get(w,1) for w in tok] + [0]*(self.max_len- len(tok))\n",
    "        return ids\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        s1, s2, label = self.data[i]\n",
    "        return torch.tensor(self.encode(s1)), torch.tensor(self.encode(s2)), torch.tensor(label)\n",
    "# 从一个 .jsonl 格式的文件中读取句子对和标签\n",
    "def load_jsonl_pair(file_path):\n",
    "    \"\"\"\n",
    "    读取 SNLI 数据集格式的 jsonl 文件，并返回三元组：(sentence1, sentence2, label)\n",
    "    标签映射：entailment → 0, neutral → 1, contradiction → 2\n",
    "    \"\"\"\n",
    "    label_map = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            d = json.loads(line)\n",
    "            label = d[\"gold_label\"]\n",
    "            if label not in label_map:\n",
    "                continue  # 忽略无效标签\n",
    "            sent1 = d[\"sentence1\"]\n",
    "            sent2 = d[\"sentence2\"]\n",
    "            data.append((sent1, sent2, label_map[label]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bf05b",
   "metadata": {},
   "source": [
    "## 构建经典文本匹配模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a30c7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=100, hid=128, num_labels=3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.bilstm1 = nn.LSTM(emb_dim, hid, bidirectional=True, batch_first=True)\n",
    "        self.bilstm2 = nn.LSTM(hid*8, hid, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hid*8, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hid, num_labels)\n",
    "        )\n",
    "    def forward(self, x1, x2):\n",
    "        emb1, emb2 = self.embed(x1), self.embed(x2)\n",
    "        o1, _ = self.bilstm1(emb1)\n",
    "        o2, _ = self.bilstm1(emb2)\n",
    "        # 注意力机制\n",
    "        sim = torch.bmm(o1, o2.transpose(1,2))\n",
    "        a2 = F.softmax(sim, dim=2)\n",
    "        a1 = F.softmax(sim.transpose(1,2), dim=2)\n",
    "        m1 = torch.bmm(a2, o2)\n",
    "        m2 = torch.bmm(a1, o1)\n",
    "        # 将原始表示和对齐信息组合起来，以捕捉推理线索\n",
    "        comp1 = torch.cat([o1, m1, o1-m1, o1*m1], dim=-1)\n",
    "        comp2 = torch.cat([o2, m2, o2-m2, o2*m2], dim=-1)\n",
    "        # 推理\n",
    "        o1c, _ = self.bilstm2(comp1)\n",
    "        o2c, _ = self.bilstm2(comp2)\n",
    "        # 池化 + 拼接 + 分类\n",
    "        v1 = torch.cat([o1c.mean(1), o1c.max(1)[0]], dim=-1)\n",
    "        v2 = torch.cat([o2c.mean(1), o2c.max(1)[0]], dim=-1)\n",
    "        out = torch.cat([v1, v2], dim=-1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053de4f",
   "metadata": {},
   "source": [
    "## 训练与检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77ee920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, opt, crit, device, num_epochs=10):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x1, x2, y in loader:\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            logit = model(x1, x2)\n",
    "            loss = crit(logit, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        return avg_loss\n",
    "'''在验证集上评估模型'''\n",
    "def eval_epoch(model, loader, crit, device):\n",
    "    model.eval()\n",
    "    tot_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x1,x2,y in loader:\n",
    "            x1,x2,y = x1.to(device), x2.to(device), y.to(device)\n",
    "            logit = model(x1,x2)\n",
    "            tot_loss += crit(logit,y).item()\n",
    "            pred = logit.argmax(1)\n",
    "            correct += (pred==y).sum().item()\n",
    "    return tot_loss/len(loader), correct/len(loader.dataset)\n",
    "\n",
    "'''句子对判断'''\n",
    "# 给定两个句子，使用训练好的 ESIM 模型预测它们之间的逻辑关系（蕴含、中立、矛盾）\n",
    "def infer_relation(model, sent1, sent2, word2idx, device, max_len=50):\n",
    "    model.eval()\n",
    "    def encode(s):\n",
    "        tok = tokenize(s)[:max_len]\n",
    "        ids = [word2idx.get(w, 1) for w in tok] + [0] * (max_len - len(tok))\n",
    "        return torch.tensor(ids).unsqueeze(0).to(device)\n",
    "    x1 = encode(sent1)\n",
    "    x2 = encode(sent2)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x1, x2)\n",
    "        pred = logits.argmax(1).item()\n",
    "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "    return label_map[pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795540c7",
   "metadata": {},
   "source": [
    "## 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6827f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 0.7261 | Test Loss: 0.6092 | Test Acc: 0.7439\n",
      "Epoch  1 | Train Loss: 0.5903 | Test Loss: 0.5596 | Test Acc: 0.7720\n",
      "Epoch  2 | Train Loss: 0.5281 | Test Loss: 0.5415 | Test Acc: 0.7799\n",
      "Epoch  3 | Train Loss: 0.4753 | Test Loss: 0.5396 | Test Acc: 0.7831\n",
      "Epoch  4 | Train Loss: 0.4259 | Test Loss: 0.5708 | Test Acc: 0.7848\n",
      "Epoch  5 | Train Loss: 0.3761 | Test Loss: 0.5869 | Test Acc: 0.7809\n",
      "Epoch  6 | Train Loss: 0.3296 | Test Loss: 0.6507 | Test Acc: 0.7778\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    '''数据读取'''\n",
    "    train_pairs = load_jsonl_pair(\"D:/Data/master/snli_1.0/snli_1.0_train.jsonl\")\n",
    "    test_pairs = load_jsonl_pair(\"D:/Data/master/snli_1.0/snli_1.0_test.jsonl\")\n",
    "    word2idx = build_vocab(train_pairs, max_vocab=10000)\n",
    "    # 创建 Dataset 和 DataLoader\n",
    "    train_ds = PairDataset(train_pairs, word2idx)\n",
    "    test_ds = PairDataset(test_pairs, word2idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64)\n",
    "    '''模型准备'''\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model3 = ESIM(len(word2idx), emb_dim=100, hid=128, num_labels=3).to(device)\n",
    "    opt = torch.optim.Adam(model3.parameters(), lr=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', factor=0.5, patience=2)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    '''训练循环 + Early Stopping'''\n",
    "    best_loss, patience = float('inf'), 3\n",
    "    wait = 0\n",
    "    best_state = None\n",
    "    for epoch in range(20):\n",
    "        trl = train(model3,train_loader,opt,crit,device)\n",
    "        vll, vac = eval_epoch(model3,test_loader,crit,device)\n",
    "        print(f\"Epoch {epoch:2d} | Train Loss: {trl:.4f} | Test Loss: {vll:.4f} | Test Acc: {vac:.4f}\")\n",
    "        scheduler.step(vll)\n",
    "        if vll < best_loss:\n",
    "            best_loss, best_state = vll, model3.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "    # 加载最优模型\n",
    "    if best_state:\n",
    "        model3.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d46c4b1",
   "metadata": {},
   "source": [
    "# 基于LSTM+CRF的标签序列\n",
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "815d38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''从 CoNLL 格式的文件中读取数据'''\n",
    "# sents: 每个句子是一个单词列表，如 [\"EU\", \"rejects\", \"German\", \"call\"]\n",
    "# tags: 每个句子对应的NER标签序列，如 [\"B-ORG\", \"O\", \"B-MISC\", \"O\"]\n",
    "def read_conll(fpath):\n",
    "    sents, tags = [], []\n",
    "    sent, tag = [], []\n",
    "    for line in open(fpath, encoding='utf8'):\n",
    "        line = line.strip()\n",
    "        if not line:  # 跳过空行\n",
    "            if sent:  # 如果当前句子非空，则保存\n",
    "                sents.append(sent)\n",
    "                tags.append(tag)\n",
    "                sent, tag = [], []\n",
    "            continue\n",
    "        try:\n",
    "            token, pos, chunk, ner = line.split()\n",
    "            sent.append(token)\n",
    "            tag.append(ner)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping invalid line: {line}\")  # 可以打印出问题行，便于调试\n",
    "    return sents, tags\n",
    "'''构建词表（word2idx）：把词转成数字索引'''\n",
    "def build_vocab(seqs, min_freq=1):\n",
    "    cnt = defaultdict(int)\n",
    "    for seq in seqs:\n",
    "        for w in seq:\n",
    "            cnt[w] += 1\n",
    "    return {w:i+2 for i,(w,c) in enumerate(cnt.items()) if c>=min_freq} | {'<PAD>':0,'<UNK>':1}\n",
    "'''构建标签的编码表'''\n",
    "def build_vocab(seqs, max_vocab=10000, min_freq=1):\n",
    "    cnt = Counter()\n",
    "    for seq in seqs:  # 直接遍历句子列表\n",
    "        cnt.update(seq)  # 统计词频\n",
    "    vocab = [w for w, freq in cnt.items() if freq >= min_freq][:max_vocab-2]\n",
    "    word2idx = {w: i+2 for i, w in enumerate(vocab)}\n",
    "    word2idx[\"<PAD>\"], word2idx[\"<UNK>\"] = 0, 1  # 确保包含<UNK>和<PAD>\n",
    "    return word2idx\n",
    "'''将每个句子中的词或标签编码成数字索引，并统一长度'''\n",
    "def encode_and_pad(seqs, vocab, pad_len):\n",
    "    if '<UNK>' not in vocab:\n",
    "#         print(\"Warning: '<UNK>' is missing in vocab!\")\n",
    "        vocab['<UNK>'] = 1    # 如果`<UNK>`没有在 vocab 中，手动添加\n",
    "    if '<PAD>' not in vocab:\n",
    "        vocab['<PAD>'] = 0  # 如果 vocab 中没有 <PAD>，则手动添加\n",
    "    idxs = [[vocab.get(w, vocab['<UNK>']) for w in s] for s in seqs]\n",
    "    return [seq + [vocab['<PAD>']]*(pad_len-len(seq)) for seq in idxs]\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sents, tags, w2i, t2i, pad_len):\n",
    "        self.X = torch.tensor(encode_and_pad(sents, w2i, pad_len), dtype=torch.long)\n",
    "        self.Y = torch.tensor(encode_and_pad(tags, t2i, pad_len), dtype=torch.long)\n",
    "        self.mask = (self.X!=w2i['<PAD>']).long()\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.mask[i], self.Y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effd86c",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f31d7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size, emb_dim=100, hid=128):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.bilstm = nn.LSTM(emb_dim, hid, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hid*2, tag_size)\n",
    "        self.crf = CRF(tag_size)\n",
    "    def forward(self, X, mask, tags=None):\n",
    "        emb = self.emb(X)\n",
    "        h, _ = self.bilstm(emb)\n",
    "        feats = self.fc(h)\n",
    "#         print(\"feats shape:\", feats.shape)\n",
    "        if tags is not None:\n",
    "            return -self.crf(feats, tags, mask)  # 负 log-likelihood\n",
    "        return self.crf.decode(feats, mask)\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self, tag_size):\n",
    "        super().__init__()\n",
    "        self.tag_size = tag_size\n",
    "        self.transitions = nn.Parameter(torch.randn(tag_size, tag_size))\n",
    "        self.start = nn.Parameter(torch.randn(tag_size))\n",
    "        self.end = nn.Parameter(torch.randn(tag_size))\n",
    "    def forward(self, feats, tags, mask):\n",
    "        # 前向-后向算法求 partition + score\n",
    "        return self._neg_log_likelihood(feats, tags, mask)\n",
    "    def _neg_log_likelihood(self, feats, tags, mask):\n",
    "        \"\"\"\n",
    "        计算负对数似然：CRF的损失函数，使用前向-后向算法\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = feats.shape\n",
    "        log_likelihood = torch.zeros(batch_size).to(feats.device)\n",
    "        # 遍历每个句子，计算每个句子的得分\n",
    "        for i in range(batch_size):\n",
    "            mask_i = mask[i]  # 获取当前句子的mask（去除padding的部分）\n",
    "            tags_i = tags[i, mask_i == 1]  # 当前句子的标签（去除padding）\n",
    "            feats_i = feats[i, mask_i == 1]  # 当前句子的特征（去除padding）\n",
    "            # 计算CRF的得分\n",
    "            score = self.start[tags_i[0]] + feats_i[0, tags_i[0]]  # 第一个标签的得分\n",
    "            for t in range(1, len(tags_i)):\n",
    "                score += feats_i[t, tags_i[t]] + self.transitions[tags_i[t-1], tags_i[t]]\n",
    "            # 使用前向-后向算法计算分区函数（partition function）\n",
    "            partition = self._forward_backward(feats_i, mask_i)\n",
    "            log_likelihood[i] = score - partition\n",
    "        return -log_likelihood.sum()\n",
    "    \n",
    "    def _forward_backward(self, feats, mask):\n",
    "        \"\"\"\n",
    "        使用前向-后向算法计算分区函数（logZ）。\n",
    "        \"\"\"\n",
    "        seq_len, tag_size = feats.shape\n",
    "        alpha = torch.full((seq_len, tag_size), -10000.0).to(feats.device)  # 初始化alpha\n",
    "        alpha[0] = self.start + feats[0]  # 初始化第一个位置的alpha值\n",
    "        # 计算每个位置的alpha值\n",
    "        for t in range(1, seq_len):\n",
    "            emit = feats[t].unsqueeze(0)         # (1, tag_size)\n",
    "            trans = self.transitions             # (tag_size, tag_size)\n",
    "            prev_alpha = alpha[t - 1].unsqueeze(1)  # (tag_size, 1)\n",
    "            alpha[t] = torch.logsumexp(prev_alpha + trans + emit, dim=0)\n",
    "        # 最终的分区函数是最后一个时间步的alpha的log值\n",
    "        return torch.logsumexp(alpha[mask.sum()-1], dim=0)\n",
    "    \n",
    "    def decode(self, feats, mask):\n",
    "        \"\"\"\n",
    "        使用Viterbi算法解码，返回最可能的标签序列。\n",
    "        feats: (B, T, C)\n",
    "        mask: (B, T)\n",
    "        return: List of predicted tag sequences\n",
    "        \"\"\"\n",
    "        B, T, C = feats.shape\n",
    "        paths = []\n",
    "        for i in range(B):\n",
    "            length = mask[i].sum()\n",
    "            score = self.start + feats[i, 0]  # (C,)\n",
    "            history = []\n",
    "            for t in range(1, length):\n",
    "                emit = feats[i, t]  # (C,)\n",
    "                next_score = []\n",
    "                backpointer = []\n",
    "                for curr in range(C):\n",
    "                    trans_score = self.transitions[:, curr] + score  # (C,)\n",
    "                    best_prev = torch.argmax(trans_score)\n",
    "                    backpointer.append(best_prev.item())\n",
    "                    next_score.append(trans_score[best_prev] + emit[curr])\n",
    "                score = torch.stack(next_score)\n",
    "                history.append(backpointer)\n",
    "            # 终止 + 回溯\n",
    "            score += self.end\n",
    "            best_last = torch.argmax(score).item()\n",
    "            best_path = [best_last]\n",
    "            for back in reversed(history):\n",
    "                best_last = back[best_last]\n",
    "                best_path.append(best_last)\n",
    "            best_path.reverse()\n",
    "            paths.append(best_path)\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fddea6",
   "metadata": {},
   "source": [
    "## 训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "18f8614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, mask, Y in loader:\n",
    "        opt.zero_grad()\n",
    "        loss = model(X.to(device), mask.to(device), Y.to(device))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, mask, Y in loader:\n",
    "            preds = model(X.to(device), mask.to(device))   # 使用decode方法来获取预测标签\n",
    "            for i, length in enumerate(mask.sum(1)):\n",
    "                y_true += Y[i,:length].tolist()\n",
    "                y_pred += preds[i][:length]\n",
    "    p,r,f,_ = precision_recall_fscore_support(y_true, y_pred, average='micro', labels=list(label2idx.values()))\n",
    "    return p, r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79b522",
   "metadata": {},
   "source": [
    "## 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15df0b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22']]\n",
      "[['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.'], ['Nadim', 'Ladki'], ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06']]\n"
     ]
    }
   ],
   "source": [
    "print(train_s[:3])  # 查看前3条数据\n",
    "print(dev_s[:3])  # 查看前3条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9162f225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=-57469.3476, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 2: loss=-161807.7289, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 3: loss=-262519.7292, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 4: loss=-362364.0781, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 5: loss=-461796.0836, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 6: loss=-561025.6654, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 7: loss=-660182.7294, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 8: loss=-759252.2223, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 9: loss=-858291.7232, P=0.0359, R=0.0684, F1=0.0471\n",
      "Epoch 10: loss=-957208.0310, P=0.0359, R=0.0684, F1=0.0471\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_s, train_t = read_conll(r\"D:\\Data\\master\\Conll-2003\\train.txt\")\n",
    "dev_s, dev_t = read_conll(r\"D:\\Data\\master\\Conll-2003\\test.txt\")\n",
    "w2i = build_vocab(train_s + dev_s)\n",
    "label2idx, idx2label = build_label_vocab([tag for seq in train_t+dev_t for tag in seq])\n",
    "pad_len = max(map(len, train_s + dev_s))\n",
    "train_ds = NERDataset(train_s, train_t, w2i, label2idx, pad_len)\n",
    "dev_ds = NERDataset(dev_s, dev_t, w2i, label2idx, pad_len)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=32)\n",
    "\n",
    "model4 = BiLSTM_CRF(len(w2i), len(label2idx)).to(device)\n",
    "optimizer = optim.Adam(model4.parameters(), lr=0.001)\n",
    "for epoch in range(10):\n",
    "    train_loss = train_epoch(model4, train_loader, optimizer)\n",
    "    p, r, f = evaluate(model4, dev_loader)\n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss:.4f}, P={p:.4f}, R={r:.4f}, F1={f:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372aa33b",
   "metadata": {},
   "source": [
    "# 基于神经网络的语言模型\n",
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e4dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取原始文本\n",
    "with open(\"D:/Data/master/poetryFromTang.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# 构建字符字典\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "# 定义编码/解码函数\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(t): return ''.join([itos[i] for i in t])\n",
    "# 整体文本转为数字序列\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# 设置模型训练的超参数\n",
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b617de",
   "metadata": {},
   "source": [
    "## 自定义数据集\n",
    "> 将长文本数据切分为多个小段，并用 DataLoader 批量加载，供语言模型进行训练\n",
    "1. 语言模型是序列任务，需要“窗口式”切分\n",
    "2. 神经网络是批量训练的\n",
    "3. 可以更灵活控制训练样本数、shuffle、batch size 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d7519ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按序列切分文本数据\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    def __getitem__(self, i):\n",
    "        return (self.data[i:i+self.seq_len],\n",
    "                self.data[i+1:i+self.seq_len+1])\n",
    "\n",
    "dataset = CharDataset(data, SEQ_LEN)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b03d6d",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43945b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, model='lstm'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        rnn_cls = nn.LSTM if model == 'lstm' else nn.GRU\n",
    "        self.rnn = rnn_cls(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42297b3",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f8bbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, epochs=10, device='cpu'):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        perplexity = np.exp(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "'''生成文本'''\n",
    "def generate(model, start_seq=\"春\", length=100, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor([stoi.get(ch, 1) for ch in start_seq], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    generated = list(start_seq)\n",
    "    hidden = None\n",
    "    for _ in range(length):\n",
    "        logits, hidden = model(input_seq, hidden)\n",
    "        probs = torch.softmax(logits[:, -1], dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(itos[next_id])\n",
    "        input_seq = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "    return ''.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f604d243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.5250, Perplexity: 33.95\n",
      "Epoch 2, Loss: 0.4739, Perplexity: 1.61\n",
      "Epoch 3, Loss: 0.1441, Perplexity: 1.15\n",
      "Epoch 4, Loss: 0.0970, Perplexity: 1.10\n",
      "Epoch 5, Loss: 0.0779, Perplexity: 1.08\n",
      "Epoch 6, Loss: 0.0670, Perplexity: 1.07\n",
      "Epoch 7, Loss: 0.0603, Perplexity: 1.06\n",
      "Epoch 8, Loss: 0.0556, Perplexity: 1.06\n",
      "Epoch 9, Loss: 0.0524, Perplexity: 1.05\n",
      "Epoch 10, Loss: 0.0502, Perplexity: 1.05\n",
      "\n",
      "🌸 生成诗句示例：\n",
      "春风颇愁。惜哉瑶池饮，日晏昆仑丘。\n",
      "黄鹄去不息，哀鸣何所投。君看随阳雁，各有稻粱谋。\n",
      "\n",
      "平明跨驴出，未知适谁门。权门多噂eR，且复寻诸孙。\n",
      "诸孙贫无事，宅舍如荒村。堂前自生竹，堂后自生萱。\n",
      "萱草秋已死，竹\n"
     ]
    }
   ],
   "source": [
    "'''运行'''\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CharRNN(vocab_size, model='lstm')  # 或 'gru'\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model, loader, optimizer, criterion, epochs=10, device=device)\n",
    "\n",
    "print(\"\\n🌸 生成诗句示例：\")\n",
    "print(generate(model, start_seq=\"春风\", length=100, device=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
